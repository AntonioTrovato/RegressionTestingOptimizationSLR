@article{SHAH201998,
title = {A feature-based soft sensor for spectroscopic data analysis},
journal = {Journal of Process Control},
volume = {78},
pages = {98-107},
year = {2019},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2019.03.016},
url = {https://www.sciencedirect.com/science/article/pii/S095915241830146X},
author = {Devarshi Shah and Jin Wang and Q. Peter He},
keywords = {Soft sensor, Variable selection, Multivariate regression, Partial least squares, Kernel partial least squares, Statistics pattern analysis, NIR, UV/Vis, Chemometrics},
abstract = {In the last few decades, spectroscopic techniques such as near-infrared (NIR) and UV/vis spectroscopies have gained wide applications. As a result, various soft sensors have been developed to predict sample properties from its spectroscopic readings. Because the readings at different wavelengths are highly correlated, it has been shown that variable selection could significantly improve a soft sensor’s prediction performance and reduce the model complexity. Currently, almost all variable selection methods focus on how to select the variables (i.e., wavelengths or wavelength segments) that are strongly correlated with the dependent variable to improve the prediction performance. Although many successful applications have been reported, such variable selection methods do have their limitations, such as high sensitivity to the choice of training data, and deteriorated performance when testing on new samples. One possible reason is the removal of useful wavelengths or segments of wavelengths during the calibration process, which could be “tilted” to overfit or capture the noise or unknown disturbances contained in the calibration data. As a result, the model prediction performance may deteriorate significantly when the model is extrapolated or applied to new samples. To address this limitation, we propose a feature-based soft sensor approach utilizing statistics pattern analysis (SPA). Instead of selecting certain wavelengths or wavelength segments, the SPA-based method considers the whole spectrum which is divided into segments, and extracts different features over each spectrum segment to build the soft sensor. In other words, the SPA model contains the complete information from the full spectrum without any selection or removal, which we believe is the main reason for the high robustness of the SPA-based method. In addition, we propose a Monte Carlo validation and testing (MCVT) procedure and three MCVT-based performance indices for consistent and fair comparison of different soft sensor methods across different datasets. The MCVT procedure and indices are generally applicable for model comparison in other applications. Four case studies are presented to demonstrate the performance of the feature-based soft sensor and to compare it with a full partial least squares (PLS), a least absolute shrinkage and selection operator (Lasso), and a synergy interval PLS (SiPLS) based models following the proposed MCVT procedure. In addition, we examine the potential of kernel PLS (KPLS) based soft sensor approaches, examine their performances, and discuss their pros and cons.}
}
@article{ZHOU2017199,
title = {A sequential multi-fidelity metamodeling approach for data regression},
journal = {Knowledge-Based Systems},
volume = {134},
pages = {199-212},
year = {2017},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2017.07.033},
url = {https://www.sciencedirect.com/science/article/pii/S0950705117303556},
author = {Qi Zhou and Yan Wang and Seung-Kyum Choi and Ping Jiang and Xinyu Shao and Jiexiang Hu},
keywords = {Multi-fidelity information, Gaussian process model, Sequential design, Prediction accuracy},
abstract = {Multi-fidelity (MF) metamodeling approaches have attracted significant attention recently for data regression because they can make a trade-off between high accuracy and low computational expense by integrating the information from high-fidelity (HF) and low-fidelity (LF) models. To facilitate the usage of the MF metamodeling approaches, there are still challenging issues on the sample size ratio between HF and LF models and the locations of samples since these two components have profound effects on the prediction accuracy of the MF metamodels. In this study, a sequential multi-fidelity (SMF) metamodeling approach is proposed to address the issues of 1) where to allocate the LF and HF sample points, and 2) how to obtain an optimal combination of the high and low-fidelity sample sizes for a given computational budget and a high-to-low simulation cost ratio. Firstly, sequential objective formulations, with the objective to reduce the estimation of prediction error of MF metamodel, are constructed to update the LF and HF sampling data. Secondly, a decision criterion is proposed to determine whether one HF experiment or several LF experiments with the equivalent computational cost should be selected to update the MF metamodel. The proposed criterion is developed according to which selection will have a greater potential value to improve the prediction accuracy of the MF metamodel. To demonstrate the effectiveness and merits of the proposed SMF metamodeling approach, two numerical examples and a practical aerospace application example are used. Results show that the proposed approach can generate more accurate MF metamodels by providing the optimal high-to-low sample size ratio and sample locations.}
}
@article{LIANG20131519,
title = {An edge detection with automatic scale selection approach to improve coherent visual attention model},
journal = {Pattern Recognition Letters},
volume = {34},
number = {13},
pages = {1519-1524},
year = {2013},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2013.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167865513002298},
author = {Jiayu Liang and Shiu Yin Yuen},
keywords = {Edge detection, Scale selection, Visual attention, Biological vision},
abstract = {An automatic scale selection approach is developed to improve the coherent visual attention model (Le Meur, O., Le Callet, P., Barba, D., Thoreau, D., 2006. A coherent computational approach to model bottom-up visual attention. IEEE Trans. Pattern Anal. Machine Intell. 28 (5), 802–817). The new approach uses linear regression to combine the automatic scale selection attention model with the coherent visual attention model. It is biologically more plausible because two important properties (i.e. edge detection and scale selection) of human vision are taken into account. Its performance is evaluated using a large human fixation dataset. The t-test indicates that the improved model outperforms the coherent visual attention model highly significantly in both the non-weighting and weighting cases. The new model also outperforms seven other state-of-the-art saliency prediction models highly significantly (p<0.01). Thus it furnishes a more accurate model for human visual attention prediction.}
}
@article{FARSHAD2014127,
title = {Transmission line fault location using hybrid wavelet-Prony method and relief algorithm},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {61},
pages = {127-136},
year = {2014},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2014.03.045},
url = {https://www.sciencedirect.com/science/article/pii/S0142061514001471},
author = {Mohammad Farshad and Javad Sadeh},
keywords = {Artificial intelligence, Fault location, Feature extraction, Feature selection, Transmission line},
abstract = {Context: Intelligent fault locating in transmission lines consists of three main steps: feature extraction, feature selection, and utilizing a learning tool. Objective: The main objective of this paper is to propose a systematic approach for intelligent fault locating in transmission lines. Method: This paper extracts a group of candidate features by applying a combination of the Wavelet Packet Decomposition (WPD) and Improved Prony Analysis (IPA) methods on single-ended voltage measurements. To have an accurate fault location estimate, useful and efficient features are selected among the candidate features using the regression relief algorithm. In this paper, performances of three regression learning tools including the Generalized Regression Neural Network (GRNN), k-Nearest Neighbor (k-NN) and the Random Forests (RF) in the fault location problem are evaluated and compared, and the best tool is introduced. Results: Numerous training and test patterns are generated through simulation of various fault types in an untransposed transmission line based on different values of fault location, fault resistance, fault inception angle, and magnitude and direction of load current. The results of evaluation using theses patterns show the high efficiency and accuracy of the proposed approach. For various fault types in the test cases, the average values of fault location estimation errors are in the range of 0.153–0.202%. Conclusion: Besides accuracy, the proposed fault locating method is immune against current signal measurement errors and it does not face the problems and costs related to the transmitting and synchronizing data of both line ends.}
}
@article{XU201625,
title = {A selective fuzzy ARTMAP ensemble and its application to the fault diagnosis of rolling element bearing},
journal = {Neurocomputing},
volume = {182},
pages = {25-35},
year = {2016},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2015.12.015},
url = {https://www.sciencedirect.com/science/article/pii/S0925231215019359},
author = {Zengbing Xu and Yourong Li and Zhigang Wang and Jianping Xuan},
keywords = {Modified distance discriminant technique, Fuzzy ARTMAP, Correlation measure, Bayesian belief method, Selective ensemble of multiple classifiers, Fault diagnosis},
abstract = {A novel intelligent fault diagnosis method based on feature extraction methods, features selection using modified distance discriminant technique and selective ensemble of multiple fuzzy ARTMAP (FAM) classifiers is proposed in this paper. The method consists of three stages. Firstly, different features in multiple symptom domains, such as time-domain features, frequency-domain features, wavelet grey moments, wavelet packet energy spectrum and auto-regression model parameters, are extracted from the raw vibration signals. Secondly, with the modified distance discriminant technique five salient feature sets are selected from the five original feature sets in different symptom domains respectively. Finally, these optimal feature sets are input the selective ensemble of multiple FAM classifiers based on the correlation measure method and Bayesian belief method to identify different abnormal cases. The proposed method is applied to the fault diagnosis of rolling element bearings, the test result shows that the selective ensemble of four FAM classifiers can identify the different fault conditions accurately and has a better classification performance compared to the single FAM and ensemble of all FAM classifiers. Besides, the diagnosis performance of the selective ensemble is analyzed by the bootstrap method. All experiment results have demonstrated that the selective ensemble of FAM classifiers has the effectiveness, stability, generalization, reliability and robustness.}
}
@article{SOUSA201484,
title = {Evaluation of artificial intelligence tool performance and uncertainty for predicting sewer structural condition},
journal = {Automation in Construction},
volume = {44},
pages = {84-91},
year = {2014},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2014.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0926580514000909},
author = {Vitor Sousa and José P. Matos and Natércia Matias},
keywords = {Artificial neural networks, Optimization, Sewer structural condition, Support vector machines},
abstract = {The implementation of a risk-informed asset management system by a wastewater infrastructure utility requires information regarding the probability and the consequences of component failures. This paper focuses on the former, evaluating the performance of artificial intelligence tools, namely artificial neural networks (ANNs) and support vector machines (SVMs), in predicting the structural condition of sewers. The performance of these tools is compared with that of logistic regression on the case study of the wastewater infrastructures of SANEST — Sistema de Saneamento da Costa do Estoril (Costa do Estoril Wastewater System). The uncertainty associated to ANNs and SVMs is quantified and the results of a trial and error approach and the use of optimization algorithms to develop SVMs are compared. The results highlight the need to account for both the performance and the uncertainty in the process of choosing the best model to estimate the sewer condition, since the ANNs present the highest average performance (78.5% correct predictions in the test sample) but also the highest dispersion of performance results (73% to 81% correct predictions in the test sample), whereas the SVMs have lower average performance (71.1% without optimization and 72.6% with the parameters optimized using the Covariance Matrix Adaptation Evolution Strategy) but little variability.}
}
@article{TOHME2020107141,
title = {A generalized Bayesian approach to model calibration},
journal = {Reliability Engineering & System Safety},
volume = {204},
pages = {107141},
year = {2020},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2020.107141},
url = {https://www.sciencedirect.com/science/article/pii/S0951832020306426},
author = {Tony Tohme and Kevin Vanslette and Kamal Youcef-Toumi},
keywords = {Calibration and Validation, Reliability and Safety, Regression, Bayesian Validation Metric, Bayesian Model Testing, Bayesian Probability Theory, Inference},
abstract = {In model development, model calibration and validation play complementary roles toward learning reliable models. In this article, we expand the Bayesian Validation Metric framework to a general calibration and validation framework by inverting the validation mathematics into a generalized Bayesian method for model calibration and regression. We perform Bayesian regression based on a user’s definition of model-data agreement. This allows for model selection on any type of data distribution, unlike Bayesian and standard regression techniques, that “fail” in some cases. We show that our tool is capable of representing and combining least squares, likelihood-based, and Bayesian calibration techniques in a single framework while being able to generalize aspects of these methods. This tool also offers new insights into the interpretation of the predictive envelopes (also known as confidence bands) while giving the analyst more control over these envelopes. We demonstrate the validity of our method by providing three numerical examples to calibrate different models, including a model for energy dissipation in lap joints under impact loading. By calibrating models with respect to the validation metrics one desires a model to ultimately pass, reliability and safety metrics may be integrated into and automatically adopted by the model in the calibration phase.}
}
@article{ALAMANIOTIS2014188,
title = {Regression to fuzziness method for estimation of remaining useful life in power plant components},
journal = {Mechanical Systems and Signal Processing},
volume = {48},
number = {1},
pages = {188-198},
year = {2014},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2014.02.014},
url = {https://www.sciencedirect.com/science/article/pii/S0888327014000727},
author = {Miltiadis Alamaniotis and Austin Grelle and Lefteri H. Tsoukalas},
keywords = {RUL, Fuzzy sets, Regression, Power plant components},
abstract = {Mitigation of severe accidents in power plants requires the reliable operation of all systems and the on-time replacement of mechanical components. Therefore, the continuous surveillance of power systems is a crucial concern for the overall safety, cost control, and on-time maintenance of a power plant. In this paper a methodology called regression to fuzziness is presented that estimates the remaining useful life (RUL) of power plant components. The RUL is defined as the difference between the time that a measurement was taken and the estimated failure time of that component. The methodology aims to compensate for a potential lack of historical data by modeling an expert׳s operational experience and expertise applied to the system. It initially identifies critical degradation parameters and their associated value range. Once completed, the operator׳s experience is modeled through fuzzy sets which span the entire parameter range. This model is then synergistically used with linear regression and a component׳s failure point to estimate the RUL. The proposed methodology is tested on estimating the RUL of a turbine (the basic electrical generating component of a power plant) in three different cases. Results demonstrate the benefits of the methodology for components for which operational data is not readily available and emphasize the significance of the selection of fuzzy sets and the effect of knowledge representation on the predicted output. To verify the effectiveness of the methodology, it was benchmarked against the data-based simple linear regression model used for predictions which was shown to perform equal or worse than the presented methodology. Furthermore, methodology comparison highlighted the improvement in estimation offered by the adoption of appropriate of fuzzy sets for parameter representation.}
}
@article{SILHAVY20171,
title = {Analysis and selection of a regression model for the Use Case Points method using a stepwise approach},
journal = {Journal of Systems and Software},
volume = {125},
pages = {1-14},
year = {2017},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2016.11.029},
url = {https://www.sciencedirect.com/science/article/pii/S016412121630231X},
author = {Radek Silhavy and Petr Silhavy and Zdenka Prokopova},
keywords = {Software size estimation, Stepwise approach, Multiple linear regression, Use Case Points, Dataset, Variables analysis},
abstract = {This study investigates the significance of use case points (UCP) variables and the influence of the complexity of multiple linear regression models on software size estimation and accuracy. Stepwise multiple linear regression models and residual analysis were used to analyse the impact of model complexity. The impact of each variable was studied using correlation analysis. The estimated size of software depends mainly on the values of the weights of unadjusted UCP, which represent a number of use cases. Moreover, all other variables (unadjusted actors' weights, technical complexity factors, and environmental complexity factors) from the UCP method also have an impact on software size and therefore cannot be omitted from the regression model. The best performing model (Model D) contains an intercept, linear terms, and squared terms. The results of several evaluation measures show that this model's estimation ability is better than that of the other models tested. Model D also performs better when compared to the UCP model, whose Sum of Squared Error was 268,620 points on Dataset 1 and 87,055 on Dataset 2. Model D achieved a greater than 90% reduction in the Sum of Squared Errors compared to the Use Case Points method on Dataset 1 and a greater than 91% reduction on Dataset 2. The medians of the Sum of Squared Errors for both methods are significantly different at the 95% confidence level (p < 0.01), while the medians for Model D (312 and 37.26) are lower than Use Case Points (3134 and 3712) on Datasets 1 and 2, respectively.}
}
@incollection{BIANCOLILLO2019157,
title = {Chapter 6 - The Sequential and Orthogonalized PLS Regression for Multiblock Regression: Theory, Examples, and Extensions},
editor = {Marina Cocchi},
series = {Data Handling in Science and Technology},
publisher = {Elsevier},
volume = {31},
pages = {157-177},
year = {2019},
booktitle = {Data Fusion Methodology and Applications},
issn = {0922-3487},
doi = {https://doi.org/10.1016/B978-0-444-63984-4.00006-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780444639844000065},
author = {Alessandra Biancolillo and Tormod Næs},
keywords = {Classification, Multiblock regression, Multiway, SO-N-PLS, SO-N-PLS-LDA, SO-PLS, SO-PLS-LDA, Variable selection},
abstract = {In this chapter, the sequentially orthogonalized-PLS (SO-PLS) method and some of its main extensions are described and illustrated. Both theoretical aspects and applications on real data are discussed. SO-PLS is a multiblock regression method in which the information is extracted sequentially from the predictor blocks and there is no limitation in the number of predictors that can be handled. Moreover, the significance of the addition of any predictor block can be tested. An extension of the method for handling multiway arrays is also described and illustrated. SO-PLS and its extensions are versatile methods for both regression and classification; in both cases, they are particularly suitable from an interpretation point of view.}
}
@article{WILL2013168,
title = {On the use of niching genetic algorithms for variable selection in solar radiation estimation},
journal = {Renewable Energy},
volume = {50},
pages = {168-176},
year = {2013},
issn = {0960-1481},
doi = {https://doi.org/10.1016/j.renene.2012.06.039},
url = {https://www.sciencedirect.com/science/article/pii/S0960148112003904},
author = {A. Will and J. Bustos and M. Bocco and J. Gotay and C. Lamelas},
keywords = {Niching genetic algorithms, Solar radiation, Variables selection, Meteorology},
abstract = {Prediction of climatic variables, in particular those related to wind and solar radiation, has developed a huge interest in recent years, mainly due to its applications to renewable energy. In many cases there is a large number of factors that influence the climatic variable of interest, and the researcher chooses the most relevant ones (based on previous knowledge of the region, availability, etc.) and runs a series of experiments combining the available data in order to find the combination that provides the best prediction. In this work we present two applications of Niching Genetic Algorithms to solve the problem of selection of variables for the estimation of Solar Radiation. On one hand, this methodology is able to estimate a given climatic variable using databases with missing data, since the algorithm can compensate it by the use of others. On the other hand, we present a methodology that allows us to select the relevant input variables for a given climatic variable estimation or prediction problem, in a systematic way, using the same Genetic Algorithm with different parameters. Both methods were tested in the estimation of daily Global Solar Radiation in El Colmenar (Tucumán, Argentina), using linear regression on data from 14 weather stations spread along the north of Argentina. The results obtained show that the methodology is appropriate, providing an RMSE = 2.36 [MJ/m2] and R = 0.926 using an average of 64 out of 329 initial variables, on a 70 individuals/85 generations combination. For a 200 individuals/150 generations combination it obtained an RMSE = 2.34 [MJ/m2] and R = 0.928 using an average of 54 variables.}
}
@article{THALER2019108851,
title = {Sparse identification of truncation errors},
journal = {Journal of Computational Physics},
volume = {397},
pages = {108851},
year = {2019},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2019.07.049},
url = {https://www.sciencedirect.com/science/article/pii/S0021999119305352},
author = {Stephan Thaler and Ludger Paehler and Nikolaus A. Adams},
keywords = {Sparse regression, Truncation error, Modified differential equation analysis, Data-driven scientific computing, Preconditioning},
abstract = {This work presents a data-driven approach to the identification of spatial and temporal truncation errors for linear and nonlinear discretization schemes of Partial Differential Equations (PDEs). Motivated by the central role of truncation errors, for example in the creation of implicit Large Eddy schemes, we introduce the Sparse Identification of Truncation Errors (SITE) framework to automatically identify the terms of the modified differential equation from simulation data. We build on recent advances in the field of data-driven discovery and control of complex systems and combine it with classical work on modified differential equation analysis of Warming, Hyett, Lerat and Peyret. We augment a sparse regression-rooted approach with appropriate preconditioning routines to aid in the identification of the individual modified differential equation terms. The construction of such a custom algorithm pipeline allows attenuating of multicollinearity effects as well as automatic tuning of the sparse regression hyperparameters using the Bayesian information criterion (BIC). As proof of concept, we constrain the analysis to finite difference schemes and leave other numerical schemes open for future inquiry. Test cases include the linear advection equation with a forward-time, backward-space discretization, the Burgers' equation with a MacCormack predictor-corrector scheme and the Korteweg-de Vries equation with a Zabusky and Kruska discretization scheme. Based on variation studies, we derive guidelines for the selection of discretization parameters, preconditioning approaches and sparse regression algorithms. The results showcase highly accurate predictions underlining the promise of SITE for the analysis and optimization of discretization schemes, where analytic derivation of modified differential equations is infeasible.}
}
@article{CHENG2015349,
title = {A non-linear case-based reasoning approach for retrieval of similar cases and selection of target credits in LEED projects},
journal = {Building and Environment},
volume = {93},
pages = {349-361},
year = {2015},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2015.07.019},
url = {https://www.sciencedirect.com/science/article/pii/S0360132315300676},
author = {Jack C.P. Cheng and Lucky J. Ma},
keywords = {Artificial neural network, Case-based reasoning, Decision support tool, LEED-NC v2009, Non-linear, Target credit selection},
abstract = {Leadership in Energy and Environmental Design (LEED) is a widely used international green building certification program developed by the U.S. Green Building Council (USGBC). Although the need for LEED certification has grown significantly, LEED managers still face challenges in target credit selection and green building technology design. They frequently meet new types of projects with different project characteristics and requirements. Therefore, it would be helpful if LEED managers could refer to other similar certified green building cases when planning and designing LEED projects. However, this is not supported in current studies and research. This paper proposes a case-based reasoning (CBR) approach to provide case studies of similar certified green building projects and suggestions on target LEED credits. Currently, linear formation of Local-Global method is commonly used in the retrieval step of CBR. This paper presents a non-linear formation of Local-Global retrieval based on Artificial Neural Network (ANN), which can provide a higher accuracy. LEED for New Construction (LEED-NC) is the focus of this paper, and 1000 LEED-NC v2009 certified cases were collected for the case base. Pairwise comparison was conducted to generate the local distance (attribute similarity) and the target for training the ANN model. The proposed non-linear CBR approach was tested and evaluated using 20 recently certified projects, and the results showed a prediction accuracy of 80.75% on the LEED credit selection. The results were also compared with those calculated by commonly used linear CBR approaches: Multiple Regression Analysis, Correlation Analysis, and the k-NN approach. The accuracy achieved by these methods was between 71.23% and 77.54%, which was lower than the proposed approach.}
}
@article{PATRICK2015328,
title = {Subdomain-based test data generation},
journal = {Journal of Systems and Software},
volume = {103},
pages = {328-342},
year = {2015},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2014.11.033},
url = {https://www.sciencedirect.com/science/article/pii/S0164121214002647},
author = {Matthew Patrick and Rob Alexander and Manuel Oriol and John A. Clark},
keywords = {Search based testing, Input distributions, Evolution strategy},
abstract = {Considerable effort is required to test software thoroughly. Even with automated test data generation tools, it is still necessary to evaluate the output of each test case and identify unexpected results. Manual effort can be reduced by restricting the range of inputs testers need to consider to regions that are more likely to reveal faults, thus reducing the number of test cases overall, and therefore reducing the effort needed to create oracles. This article describes and evaluates search-based techniques, using evolution strategies and subset selection, for identifying regions of the input domain (known as subdomains) such that test cases sampled at random from within these regions can be used efficiently to find faults. The fault finding capability of each subdomain is evaluated using mutation analysis, a technique that is based on faults programmers are likely to make. The resulting subdomains kill more mutants than random testing (up to six times as many in one case) with the same number or fewer test cases. Optimised subdomains can be used as a starting point for program analysis and regression testing. They can easily be comprehended by a human test engineer, so may be used to provide information about the software under test and design further highly efficient test suites.}
}
@article{OKNINSKI20181,
title = {On use of hybrid rocket propulsion for suborbital vehicles},
journal = {Acta Astronautica},
volume = {145},
pages = {1-10},
year = {2018},
issn = {0094-5765},
doi = {https://doi.org/10.1016/j.actaastro.2018.01.027},
url = {https://www.sciencedirect.com/science/article/pii/S0094576517313012},
author = {Adam Okninski},
keywords = {Rocket, Sounding rocket, Hybrid rocket motor, Rocket propulsion, Optimisation, Suborbital flight},
abstract = {While the majority of operating suborbital rockets use solid rocket propulsion, recent advancements in the field of hybrid rocket motors lead to renewed interest in their use in sounding rockets. This paper presents results of optimisation of sounding rockets using hybrid propulsion. An overview of vehicles under development during the last decade, as well as heritage systems is provided. Different propellant combinations are discussed and their performance assessment is given. While Liquid Oxygen, Nitrous Oxide and Nitric Acid have been widely tested with various solid fuels in flight, Hydrogen Peroxide remains an oxidiser with very limited sounding rocket applications. The benefits of hybrid propulsion for sounding rockets are given. In case of hybrid rocket motors the thrust curve can be optimised for each flight, using a flow regulator, depending on the payload and mission. Results of studies concerning the optimal burn duration and nozzle selection are given. Specific considerations are provided for the Polish ILR-33 “Amber” sounding rocket. Low regression rates, which up to date were viewed as a drawback of hybrid propulsion may be used to the benefit of maximising rocket performance if small solid rocket boosters are used during the initial flight period. While increased interest in hybrid propulsion is present, no up-to-date reference concerning use of hybrid rocket propulsion for sounding rockets is available. The ultimate goal of the paper is to provide insight into the sensitivity of different design parameters on performance of hybrid sounding rockets and delve into the potential and challenges of using hybrid rocket technology for expendable suborbital applications.}
}
@article{DARAIO2014358,
title = {Directional distances and their robust versions: Computational and testing issues},
journal = {European Journal of Operational Research},
volume = {237},
number = {1},
pages = {358-369},
year = {2014},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2014.01.064},
url = {https://www.sciencedirect.com/science/article/pii/S0377221714001118},
author = {Cinzia Daraio and Léopold Simar},
keywords = {Directional distances, Free Disposal Hull (FDH), Conditional efficiency measures, Nonparametric frontiers, Significance test, Bootstrap},
abstract = {Directional distance functions provide very flexible tools for investigating the performance of Decision Making Units (DMUs). Their flexibility relies on their ability to handle undesirable outputs and to account for non-discretionary inputs and/or outputs by fixing zero values in some elements of the directional vector. Simar and Vanhems, 2012, Simar et al., 2012 indicate how the statistical properties of Farrell–Debreu type of radial efficiency measures can be transferred to directional distances. Moreover, robust versions of these distances are also available, for conditional and unconditional measures. Bădin, Daraio, and Simar (2012) have shown how conditional radial distances are useful to investigate the effect of environmental factors on the production process. In this paper we develop the operational aspects for computing conditional and unconditional directional distances and their robust versions, in particular when some of the elements of the directional vector are fixed at zero. After that, we show how the approach of Bădin et al. (2012) can be adapted in a directional distance framework, including bandwidth selection and two-stage regression of conditional efficiency scores. Finally, we suggest a procedure, based on bootstrap techniques, for testing the significance of environmental factors on directional efficiency scores. The procedure is illustrated through simulated and real data.}
}
@article{CARRILLO20226205,
title = {Using a statistical efficiency methodology for predictors’ selection in the bedload transport problem: A high gradient experimental channel case},
journal = {Alexandria Engineering Journal},
volume = {61},
number = {8},
pages = {6205-6219},
year = {2022},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2021.11.052},
url = {https://www.sciencedirect.com/science/article/pii/S1110016821007857},
author = {Veronica Carrillo and Daniel Mendoza and John Petrie and Pedro Matovelle and Sebastian Torres and Esteban Pacheco and Felipe Cisneros and Luis Timbe},
keywords = {High gradient, Bedload transport, Akaike-information-criterion, Best predictors, Laboratory experiments},
abstract = {Bedload transport rates for high-gradient gravel bed rivers has been studied through a physical model that replicated the typical features of these channels. A stepwise regression was performed to identify the best predictors from a set of independent variables. As independent variables channel slope, the ratio of area occupied by large particles to the total plan area, flow discharge, mean flow depth, mean flow velocity, water surface velocity, boundary shear stress, and shear velocity were considered. Different characteristic diameters (d16, d50, d84, and d90) were used to nondimensionalize the variables as well as to test the influence of grain size. A linear and a potential model were obtained for each characteristic diameter. Based on the correlation coefficients (R2) with the data used to build the models, the d50 and d84 linear and potential models were selected to perform further analysis. A set of independent data was used to verify the selected models. Better performance was observed for the potential models with 96% of the data falling within ½ order of the magnitude bands both for d50 and d84 . R2 for the d50 and d84 potential models were 0.63 and 0.76, respectively. Therefore, the d84 potential model can be selected as the present study representative model.}
}
@article{KONDO2018408,
title = {Estimation of binaural speech intelligibility using machine learning},
journal = {Applied Acoustics},
volume = {129},
pages = {408-416},
year = {2018},
issn = {0003-682X},
doi = {https://doi.org/10.1016/j.apacoust.2017.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0003682X17303304},
author = {Kazuhiro Kondo and Kazuya Taira},
keywords = {Speech intelligibility, Binaural speech, Objective estimation, Machine learning, Diagnostic rhyme test},
abstract = {We proposed and evaluated a speech intelligibility estimation method for binaural signals. The assumption here was that both the speech and competing noise are directional sources. In this case, when the speech and noise are located away from each other, the intelligibility generally improves since the auditory system can segregate these two streams. However, since intelligibility tests as well as its estimation is conducted based on monaurally-recorded signals, this potential increase in the intelligibility due to the segregation of sources is not accounted for, and the intelligibility is often under-estimated. Accordingly, in order to estimate the intelligibility taking into account this binaural advantage, we trained a mapping function between the subjective intelligibility and objective measures that account for the binaural advantage stated above. We attempted SNR calculation on (1) a simple binaural to monaural mix-down, which models the conventional estimation, (2) simple pooling of both binaural channels (pooled channel), (3) channel signal selection with the better SNR from left and right channels (better-ear), and (4) sub-band wise better-ear selection (band-wise better-ear). For the mapping function training, we tried neural networks (NN), support vector regression (SVR), and random forests (RF), and compared these to simple logistic regression (LR). We also investigated the sub-band configuration that gives the best estimation accuracy by balancing the frequency resolution and the amount of training data. It was found that the combination of the better-ear model and RF gave the best results, with root mean square error (RMSE) of about 0.11 and correlation of 0.92 in an open set test.}
}
@article{LIANG20131209,
title = {Adaptive weighted learning for linear regression problems via Kullback–Leibler divergence},
journal = {Pattern Recognition},
volume = {46},
number = {4},
pages = {1209-1219},
year = {2013},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2012.10.017},
url = {https://www.sciencedirect.com/science/article/pii/S0031320312004517},
author = {Zhizheng Liang and Youfu Li and ShiXiong Xia},
keywords = {Linear regression, KL divergence, Weighted learning, Alternative optimization, Image classification},
abstract = {In this paper, we propose adaptive weighted learning for linear regression problems via the Kullback–Leibler (KL) divergence. The alternative optimization method is used to solve the proposed model. Meanwhile, we theoretically demonstrate that the solution of the optimization algorithm converges to a stationary point of the model. In addition, we also fuse global linear regression and class-oriented linear regression and discuss the problem of parameter selection. Experimental results on face and handwritten numerical character databases show that the proposed method is effective for image classification, particularly for the case that the samples in the training and testing set have different characteristics.}
}
@incollection{PODDER2021175,
title = {9 - Application of machine learning for the diagnosis of COVID-19},
editor = {Utku Kose and Deepak Gupta and Victor Hugo C. {de Albuquerque} and Ashish Khanna},
booktitle = {Data Science for COVID-19},
publisher = {Academic Press},
pages = {175-194},
year = {2021},
isbn = {978-0-12-824536-1},
doi = {https://doi.org/10.1016/B978-0-12-824536-1.00008-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128245361000083},
author = {Prajoy Podder and Subrato Bharati and M. Rubaiyat Hossain Mondal and Utku Kose},
keywords = {Classification, COVID-19, Decision tree, Disease prediction, Logistic regression, Machine learning, Random forest},
abstract = {This chapter focuses on the application of machine learning algorithms on the diagnosis of the novel coronavirus disease (COVID-19). First, data visualization is provided on increases in confirmed deaths and recovered cases of COVID-19 using currently available data from Johns Hopkins University. Next, the machine learning algorithms are used for the automatic diagnosis of COVID-19. Data-driven diagnosis is performed using a dataset of 5644 samples with 111 attributes provided by Hospital Israelita Albert Einstein, Brazil. As a preprocessing step, null values and categorical data are processed and standardization is performed. Next, feature selection is performed to find attributes that are most important for a COVID-19 diagnosis. A number of algorithms including random forest logistic regression, XGBoost, and decision tree are considered and their kernel parameters are optimized. The performance of classification algorithms is evaluated in terms of a number of factors including the testing accuracy, precision, recall, miss rate, receiver operating characteristic curve and area under the receiver operating characteristic curve. Experimental results show that serum glucose is the most influential attribute in predicting COVID-19. Our results also show that for the case of cross-validation, XGBoost has the highest accuracy value of 92.67% and logistic regressions have the second highest accuracy of 92.58%, whereas both XGBoost and LR have a 93% value for precision, recall, and F1 score. Moreover, for the case of the holdout method with 20% testing data, logistic regression with an accuracy of 94.06% outperforms other classifiers in terms of accuracy, precision, recall, and F1 score.}
}
@article{GE2022105173,
title = {Design of a rapid diagnostic model for bladder compliance based on real-time intravesical pressure monitoring system},
journal = {Computers in Biology and Medicine},
volume = {141},
pages = {105173},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.105173},
url = {https://www.sciencedirect.com/science/article/pii/S0010482521009677},
author = {Zicong Ge and Liangfeng Tang and Yunsong Peng and Mingming Zhang and Jialong Tang and Xiaodong Yang and Yu Li and Zhongyi Wu and Gang Yuan},
abstract = {Objective
The diagnosis of bladder dysfunction for children depends on the confirmation of abnormal bladder shape and bladder compliance. The existing gold standard needs to conduct voiding cystourethrogram (VCUG) examination and urodynamic studies (UDS) examination on patients separately. To reduce the time and injury of children's inspection, we propose a novel method to judge the bladder compliance by measuring the intravesical pressure during the VCUG examination without extra UDS.
Methods
Our method consisted of four steps. We firstly developed a single-tube device that can measure, display, store, and transmit real-time pressure data. Secondly, we conducted clinical trials with the equipment on a cohort of 52 patients (including 32 negative and 20 positive cases). Thirdly, we preprocessed the data to eliminate noise and extracted features, then we used the least absolute shrinkage and selection operator (LASSO) to screen out important features. Finally, several machine learning methods were applied to classify and predict the bladder compliance level, including support vector machine (SVM), Random Forest, XGBoost, perceptron, logistic regression, and Naive Bayes, and the classification performance was evaluated.
Results
73 features were extracted, including first-order and second-order time-domain features, wavelet features, and frequency domain features. 15 key features were selected and the model showed promising classification performance. The highest AUC value was 0.873 by the SVM algorithm, and the corresponding accuracy was 84%.
Conclusion
We designed a system to quickly obtain the intravesical pressure during the VCUG test, and our classification model is competitive in judging patients’ bladder compliance.
Significance
This could facilitate rapid auxiliary diagnosis of bladder disease based on real-time data. The promising result of classification is expected to provide doctors with a reliable basis in the auxiliary diagnosis of some bladder diseases prior to UDS.}
}
@article{SEKA20199,
title = {Identification of maize (Zea mays L.) progeny genotypes based on two probabilistic approaches: Logistic regression and naïve Bayes},
journal = {Artificial Intelligence in Agriculture},
volume = {1},
pages = {9-13},
year = {2019},
issn = {2589-7217},
doi = {https://doi.org/10.1016/j.aiia.2019.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S2589721719300030},
author = {D. Seka and B.S. Bonny and A.N. Yoboué and S.R. Sié and B.A. Adopo-Gourène},
keywords = {Gaussian naïve Bayes, Logistic regression, Maize genotype, Prediction, Selection},
abstract = {We used two probabilistic methods, Gaussian Naïve Bayes and Logistic Regression to predict the genotypes of the offspring of two maize strains, the BLC and the JNE genotypes, based on the phenotypic traits of the parents. We determined the prediction performance of the two models with the overall accuracy and the area under the receiver operating curve (AUC). The overall accuracy for both models ranged between 82% and 87%. The values of the area under the receiver operating curve were 0.90 or higher for Logistic Regression models, and 0.85 or higher for Gaussian Naïve Bayes models. These statistics indicated that the two models were very effective in predicting the genotypes of the offspring. Furthermore, both models predicted the BLC genotype with higher accuracy than they did the JNE genotype. The BLC genotype appeared more homogeneous and more predictable. A Chi-square test for the homogeneity of the confusion matrices showed that in all cases the two models produced similar prediction results. That finding was in line with the assertion by Mitchell (2010) who theoretically showed that the two models are essentially the same. With logistic regression, each subset of the original data or its corresponding principal components produced exactly the same prediction results. The AUC value may be viewed as a criterion for parent-offspring resemblance for each set of phenotypic traits considered in the analysis.}
}
@incollection{MASRI201679,
title = {Chapter Four - Coverage-Based Software Testing: Beyond Basic Test Requirements},
editor = {Atif M. Memon},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {103},
pages = {79-142},
year = {2016},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2016.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0065245816300274},
author = {W. Masri and F.A. Zaraket},
keywords = {Software testing, Structural coverage, Logic coverage, User-defined coverage, Specification-based coverage, Property-based coverage, Regression testing, Profiling for coverage, Test case intent verification},
abstract = {Code coverage is one of the core quality metrics adopted by software testing practitioners nowadays. Researchers have devised several coverage criteria that testers use to assess the quality of test suites. A coverage criterion operates by: (1) defining a set of test requirements that need to be satisfied by the given test suite and (2) computing the percentage of the satisfied requirements, thus yielding a quality metric that quantifies the potential adequacy of the test suite at revealing program defects. What differentiates one coverage criterion from another is the set of test requirements involved. For example, function coverage is concerned with whether every function in the program has been called, and statement coverage is concerned with whether every statement in the program has executed. The use of code coverage in testing is not restricted to assessing the quality of test suites. For example, researchers have devised test suite minimization and test case generation techniques that also leverage coverage. Early coverage-based software testing techniques involved basic test requirements such as functions, statements, branches, and predicates, whereas recent techniques involved (1) test requirements that are complex code constructs such as paths, program dependences, and information flows or (2) test requirements that are not necessarily code constructs such as program properties and user-defined test requirements. The focus of this chapter is to compare these two generations of techniques in regard to their effectiveness at revealing defects. The chapter will first present preliminary background and definitions and then describe impactful early coverage techniques followed by selected recent work.}
}
@article{MENASSA201357,
title = {Optimizing hybrid ventilation in public spaces of complex buildings – A case study of the Wisconsin Institutes for Discovery},
journal = {Building and Environment},
volume = {61},
pages = {57-68},
year = {2013},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2012.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S0360132312003319},
author = {Carol C. Menassa and Nathan Taylor and John Nelson},
keywords = {Hybrid ventilation, Energy savings, Complex buildings, Natural ventilation, Experimental method},
abstract = {Complex buildings such as hospitals and laboratories require intensive ventilation and cooling loads in order to meet operational demands. One way to reduce energy use while meeting these demanding requirements in complex buildings is the incorporation of hybrid ventilation in areas that do not require high and continuous loads such as public spaces. This research establishes an experimental approach to test and analyze various hybrid ventilation strategies in an occupied, complex building utilizing hybrid ventilation in public spaces. To optimize the use of hybrid ventilation, this research focuses on tracking three performance criteria: energy savings, occupant comfort and indoor-air quality. The framework establishes a variety of hybrid ventilation strategies to test, and outlines how to analyze results graphically and through linear regression modeling. This experimental approach is illustrated through a case study example of a laboratory building located in Madison–Wisconsin. The selection of the ideal hybrid ventilation strategy for the public space studied resulted in 56 percent average savings in ventilation and cooling load when HV is in use, and established a potential to use hybrid ventilation for 28 percent of the 111 day cooling season (20 percent savings in mechanical cooling over the summer).}
}
@article{RIBEIRO2020105837,
title = {Ensemble approach based on bagging, boosting and stacking for short-term prediction in agribusiness time series},
journal = {Applied Soft Computing},
volume = {86},
pages = {105837},
year = {2020},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2019.105837},
url = {https://www.sciencedirect.com/science/article/pii/S1568494619306180},
author = {Matheus Henrique Dal Molin Ribeiro and Leandro {dos Santos Coelho}},
keywords = {Bagging, Boosting, Agricultural commodity, Ensemble regression, Stacking, Time series},
abstract = {The investigation of the accuracy of methods employed to forecast agricultural commodities prices is an important area of study. In this context, the development of effective models is necessary. Regression ensembles can be used for this purpose. An ensemble is a set of combined models which act together to forecast a response variable with lower error. Faced with this, the general contribution of this work is to explore the predictive capability of regression ensembles by comparing ensembles among themselves, as well as with approaches that consider a single model (reference models) in the agribusiness area to forecast prices one month ahead. In this aspect, monthly time series referring to the price paid to producers in the state of Parana, Brazil for a 60 kg bag of soybean (case study 1) and wheat (case study 2) are used. The ensembles bagging (random forests — RF), boosting (gradient boosting machine — GBM and extreme gradient boosting machine — XGB), and stacking (STACK) are adopted. The support vector machine for regression (SVR), multilayer perceptron neural network (MLP) and K-nearest neighbors (KNN) are adopted as reference models. Performance measures such as mean absolute percentage error (MAPE), root mean squared error (RMSE), mean absolute error (MAE), and mean squared error (MSE) are used for models comparison. Friedman and Wilcoxon signed rank tests are applied to evaluate the models’ absolute percentage errors (APE). From the comparison of test set results, MAPE lower than 1% is observed for the best ensemble approaches. In this context, the XGB/STACK (Least Absolute Shrinkage and Selection Operator-KNN-XGB-SVR) and RF models showed better performance for short-term forecasting tasks for case studies 1 and 2, respectively. Better APE (statistically smaller) is observed for XGB/STACK and RF in relation to reference models. Besides that, approaches based on boosting are consistent, providing good results in both case studies. Alongside, a rank according to the performances is: XGB, GBM, RF, STACK, MLP, SVR and KNN. It can be concluded that the ensemble approach presents statistically significant gains, reducing prediction errors for the price series studied. The use of ensembles is recommended to forecast agricultural commodities prices one month ahead, since a more assertive performance is observed, which allows to increase the accuracy of the constructed model and reduce decision-making risk.}
}
@article{SHARAFODDINI2019,
title = {A New Insight Into Missing Data in Intensive Care Unit Patient Profiles: Observational Study},
journal = {JMIR Medical Informatics},
volume = {7},
number = {1},
year = {2019},
issn = {2291-9694},
doi = {https://doi.org/10.2196/11605},
url = {https://www.sciencedirect.com/science/article/pii/S2291969419000048},
author = {Anis Sharafoddini and Joel A Dubin and David M Maslove and Joon Lee},
keywords = {electronic health records, clinical laboratory tests, machine learning, hospital mortality},
abstract = {Background
The data missing from patient profiles in intensive care units (ICUs) are substantial and unavoidable. However, this incompleteness is not always random or because of imperfections in the data collection process.
Objective
This study aimed to investigate the potential hidden information in data missing from electronic health records (EHRs) in an ICU and examine whether the presence or missingness of a variable itself can convey information about the patient health status.
Methods
Daily retrieval of laboratory test (LT) measurements from the Medical Information Mart for Intensive Care III database was set as our reference for defining complete patient profiles. Missingness indicators were introduced as a way of representing presence or absence of the LTs in a patient profile. Thereafter, various feature selection methods (filter and embedded feature selection methods) were used to examine the predictive power of missingness indicators. Finally, a set of well-known prediction models (logistic regression [LR], decision tree, and random forest) were used to evaluate whether the absence status itself of a variable recording can provide predictive power. We also examined the utility of missingness indicators in improving predictive performance when used with observed laboratory measurements as model input. The outcome of interest was in-hospital mortality and mortality at 30 days after ICU discharge.
Results
Regardless of mortality type or ICU day, more than 40% of the predictors selected by feature selection methods were missingness indicators. Notably, employing missingness indicators as the only predictors achieved reasonable mortality prediction on all days and for all mortality types (for instance, in 30-day mortality prediction with LR, we achieved area under the curve of the receiver operating characteristic [AUROC] of 0.6836±0.012). Including indicators with observed measurements in the prediction models also improved the AUROC; the maximum improvement was 0.0426. Indicators also improved the AUROC for Simplified Acute Physiology Score II model—a well-known ICU severity of illness score—confirming the additive information of the indicators (AUROC of 0.8045±0.0109 for 30-day mortality prediction for LR).
Conclusions
Our study demonstrated that the presence or absence of LT measurements is informative and can be considered a potential predictor of in-hospital and 30-day mortality. The comparative analysis of prediction models also showed statistically significant prediction improvement when indicators were included. Moreover, missing data might reflect the opinions of examining clinicians. Therefore, the absence of measurements can be informative in ICUs and has predictive power beyond the measured data themselves. This initial case study shows promise for more in-depth analysis of missing data and its informativeness in ICUs. Future studies are needed to generalize these results.}
}
@article{SHANBEHZADEH2022101009,
title = {Performance evaluation of machine learning for breast cancer diagnosis: A case study},
journal = {Informatics in Medicine Unlocked},
volume = {31},
pages = {101009},
year = {2022},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2022.101009},
url = {https://www.sciencedirect.com/science/article/pii/S2352914822001526},
author = {Mostafa Shanbehzadeh and Hadi Kazemi-Arpanahi and Mohammad {Bolbolian Ghalibaf} and Azam Orooji},
keywords = {Machine learning, Artificial intelligence, Data mining, Breast neoplasms},
abstract = {Introduction
Breast cancer (BC) is one of the most common and aggressive malignancies in women worldwide. It is proven that machine learning (ML) could rapidly and cost-effectively diagnose BC. This study aimed to develop and test predictive models for BC based on women's lifestyle factors using several basic and ensemble machine learning (ML) classifiers.
Methods
Data of 1503 suspected BC cases were retrospectively extracted from a hospital-based electronic database. First, important risk factors were identified using wrapper-J48, wrapper-SVM, wrapper-NB, logistic regression (LR), and correlation-based feature selection (CFS) methods. Then the performance of five basic ML algorithms, including Naïve Bayes (NB), Bayesian network (BNeT), random forest (RF), multilayer perceptron (MLP), support vector machine (SVM), C4.5, eXtreme Gradient Boosting (XGBoost), decision tree and two ensemble algorithms, including Confidence weighted voting and Voting were compared to predict BC before and after performing feature section (FS). We utilized SPSS 20 and Weka software version 3.8.4 to analyze the data. Implementation of ML models was also performed in R 3.5.0.
Results
The RF algorithm presented the best performance before and after performing FS with AUC of 0.799 and 0.798, respectively. Also, the best model's combination using the Confidence weighted voting method improved the classifier performance and achieved the best result with an 80% AUC.
Conclusions
The results showed that ensemble ML algorithms represented higher ability than basic methods. The developed models can accurately classify individuals who are at high risk for BC, and can be employed as a screening tool for the early BC detection.}
}
@article{DUMONT2015118,
title = {Thermal and hyperspectral imaging for Norway spruce (Picea abies) seeds screening},
journal = {Computers and Electronics in Agriculture},
volume = {116},
pages = {118-124},
year = {2015},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2015.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S0168169915001672},
author = {Jennifer Dumont and Tapani Hirvonen and Ville Heikkinen and Maxime Mistretta and Lars Granlund and Katri Himanen and Laure Fauch and Ilkka Porali and Jouni Hiltunen and Sarita Keski-Saari and Markku Nygren and Elina Oksanen and Markku Hauta-Kasari and Markku Keinänen},
keywords = {Classification, Hyperspectral imaging, , Thermal imaging, Seed sorting, Insect damage},
abstract = {The quality of seeds used in agriculture and forestry is tightly linked to the plant productivity. Thus, the development of high-throughput nondestructive methods to classify the seeds is of prime interest. Visible and near infrared (VNIR, 400–1000nm range) and short-wave infrared (SWIR, 1000–2500nm range) hyperspectral imaging techniques were compared to an infrared lifetime imaging technique to evaluate Norway spruce (Picea abies (L.) Karst.) seed quality. Hyperspectral image and thermal data from 1606 seeds were used to identify viable seeds, empty seeds and seeds infested by Megastigmus sp. larvae. The spectra of seeds obtained from hyperspectral imaging, especially in SWIR range and the thermal signal decay of seeds following an exposure to a short light pulse were characteristic of the seed status. Classification of the seeds to three classes was performed with a Support Vector Machine (nu-SVM) and sparse logistic regression based feature selection. Leave-One-Out classification resulted to 99% accuracy using either thermal or spectral measurements compared to radiography classification. In spectral imaging case, all important features were located in the SWIR range. Furthermore, the classification results showed that accurate (93.8%) seed sorting can be achieved with a simpler method based on information from only three hyperspectral bands at 1310nm, 1710nm and 1985nm locations, suggesting a possibility to build an inexpensive screening device. The results indicate that combined classification methods with hyperspectral imaging technique and infrared lifetime imaging technique constitute practically high performance fast and non-destructive techniques for high-throughput seed screening.}
}
@article{HEGDE2019100254,
title = {Development of non-invasive diabetes risk prediction models as decision support tools designed for application in the dental clinical environment},
journal = {Informatics in Medicine Unlocked},
volume = {17},
pages = {100254},
year = {2019},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2019.100254},
url = {https://www.sciencedirect.com/science/article/pii/S2352914819302758},
author = {Harshad Hegde and Neel Shimpi and Aloksagar Panny and Ingrid Glurich and Pamela Christie and Amit Acharya},
keywords = {Dental informatics, Decision-support systems, Electronic health records, Evidence-based practice, Machine leaning, Modeling healthcare services},
abstract = {The objective was to develop a predictive model using medical-dental data from an integrated electronic health record (iEHR) to identify individuals with undiagnosed diabetes mellitus (DM) in dental settings. Retrospective data retrieved from Marshfield Clinic Health System's data-warehouse was pre-processed prior to conducting analysis. A subset was extracted from the preprocessed dataset for external evaluation (Nvalidation) of derived predictive models. Further, subsets of 30%–70%, 40%–60% and 50%–50% case-to-control ratios were created for training/testing. Feature selection was performed on all datasets. Four machine learning (ML) classifiers were evaluated: logistic regression (LR), multilayer perceptron (MLP), support vector machines (SVM) and random forests (RF). Model performance was evaluated on Nvalidation. We retrieved a total of 5319 cases and 36,224 controls. From the initial 116 medical and dental features, 107 were used after performing feature selection. RF applied to the 50%–50% case-control ratio outperformed other predictive models over Nvalidation achieving a total accuracy (94.14%), sensitivity (0.941), specificity (0.943), F-measure (0.941), Mathews-correlation-coefficient (0.885) and area under the receiver operating curve (0.972). Future directions include incorporation of this predictive model into iEHR as a clinical decision support tool to screen and detect patients at risk for DM triggering follow-ups and referrals for integrated care delivery between dentists and physicians.}
}
@article{GAUTHIER2017134,
title = {Sound quality prediction based on systematic metric selection and shrinkage: Comparison of stepwise, lasso, and elastic-net algorithms and clustering preprocessing},
journal = {Journal of Sound and Vibration},
volume = {400},
pages = {134-153},
year = {2017},
issn = {0022-460X},
doi = {https://doi.org/10.1016/j.jsv.2017.03.025},
url = {https://www.sciencedirect.com/science/article/pii/S0022460X1730281X},
author = {Philippe-Aubert Gauthier and William Scullion and Alain Berry},
keywords = {Sound quality, Listening tests, Stepwise regression, Lasso, Elastic-net, Clustering},
abstract = {Sound quality is the impression of quality that is transmitted by the sound of a device. Its importance in sound and acoustical design of consumer products no longer needs to be demonstrated. One of the challenges is the creation of a prediction model that is able to predict the results of a listening test while using metrics derived from the sound stimuli. Often, these models are either derived using linear regression on a limited set of experimenter-selected metrics, or using more complex algorithms such as neural networks. In the former case, the user-selected metrics can bias the model and reflect the engineer pre-conceived idea of sound quality while missing potential features. In the latter case, although prediction might be efficient, the model is often in the form of a black-box which is difficult to use as a sound design guideline for engineers. In this paper, preprocessing by participants clustering and three different algorithms are compared in order to construct a sound quality prediction model that does not suffer from these limitations. The lasso, elastic-net and stepwise algorithms are tested for listening tests of consumer product for which 91 metrics are used as potential predictors. Based on the reported results, it is shown that the most promising algorithm is the lasso which is able to (1) efficiently limit the number of metrics, (2) most accurately predict the results of listening tests, and (3) provide a meaningful model that can be used as understandable design guidelines.}
}
@article{LI2014162,
title = {Selection of smoothing parameter estimators for general regression neural networks – Applications to hydrological and water resources modelling},
journal = {Environmental Modelling & Software},
volume = {59},
pages = {162-186},
year = {2014},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2014.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S1364815214001418},
author = {Xuyuan Li and Aaron C. Zecchin and Holger R. Maier},
keywords = {General regression neural networks, Smoothing parameter estimators, Artificial neural networks, Multi-layer perceptrons, Extreme and average events, Hydrology and water resources},
abstract = {Multi-layer perceptron artificial neural networks are used extensively in hydrological and water resources modelling. However, a significant limitation with their application is that it is difficult to determine the optimal model structure. General regression neural networks (GRNNs) overcome this limitation, as their model structure is fixed. However, there has been limited investigation into the best way to estimate the parameters of GRNNs within water resources applications. In order to address this shortcoming, the performance of nine different estimation methods for the GRNN smoothing parameter is assessed in terms of accuracy and computational efficiency for a number of synthetic and measured data sets with distinct properties. Of these methods, five are based on bandwidth estimators used in kernel density estimation, and four are based on single and multivariable calibration strategies. In total, 5674 GRNN models are developed and preliminary guidelines for the selection of GRNN parameter estimation methods are provided and tested.}
}
@article{HASSANZADEH2019103321,
title = {Quantifying semantic similarity of clinical evidence in the biomedical literature to facilitate related evidence synthesis},
journal = {Journal of Biomedical Informatics},
volume = {100},
pages = {103321},
year = {2019},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2019.103321},
url = {https://www.sciencedirect.com/science/article/pii/S1532046419302400},
author = {Hamed Hassanzadeh and Anthony Nguyen and Karin Verspoor},
keywords = {Clinical semantic similarity, Clinical evidence, Evidence based medicine, Systematic review},
abstract = {Objective
Published clinical trials and high quality peer reviewed medical publications are considered as the main sources of evidence used for synthesizing systematic reviews or practicing Evidence Based Medicine (EBM). Finding all relevant published evidence for a particular medical case is a time and labour intensive task, given the breadth of the biomedical literature. Automatic quantification of conceptual relationships between key clinical evidence within and across publications, despite variations in the expression of clinically-relevant concepts, can help to facilitate synthesis of evidence. In this study, we aim to provide an approach towards expediting evidence synthesis by quantifying semantic similarity of key evidence as expressed in the form of individual sentences. Such semantic textual similarity can be applied as a key approach for supporting selection of related studies.
Material and methods
We propose a generalisable approach for quantifying semantic similarity of clinical evidence in the biomedical literature, specifically considering the similarity of sentences corresponding to a given type of evidence, such as clinical interventions, population information, clinical findings, etc. We develop three sets of generic, ontology-based, and vector-space models of similarity measures that make use of a variety of lexical, conceptual, and contextual information to quantify the similarity of full sentences containing clinical evidence. To understand the impact of different similarity measures on the overall evidence semantic similarity quantification, we provide a comparative analysis of these measures when used as input to an unsupervised linear interpolation and a supervised regression ensemble. In order to provide a reliable test-bed for this experiment, we generate a dataset of 1000 pairs of sentences from biomedical publications that are annotated by ten human experts. We also extend the experiments on an external dataset for further generalisability testing.
Results
The combination of all diverse similarity measures showed stronger correlations with the gold standard similarity scores in the dataset than any individual kind of measure. Our approach reached near 0.80 average Pearson correlation across different clinical evidence types using the devised similarity measures. Although they were more effective when combined together, individual generic and vector-space measures also resulted in strong similarity quantification when used in both unsupervised and supervised models. On the external dataset, our similarity measures were highly competitive with the state-of-the-art approaches developed and trained specifically on that dataset for predicting semantic similarity.
Conclusion
Experimental results showed that the proposed semantic similarity quantification approach can effectively identify related clinical evidence that is reported in the literature. The comparison with a state-of-the-art method demonstrated the effectiveness of the approach, and experiments with an external dataset support its generalisability.}
}
@article{WEI2013109,
title = {A dynamic particle filter-support vector regression method for reliability prediction},
journal = {Reliability Engineering & System Safety},
volume = {119},
pages = {109-116},
year = {2013},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2013.05.021},
url = {https://www.sciencedirect.com/science/article/pii/S095183201300152X},
author = {Zhao Wei and Tao Tao and Ding ZhuoShu and Enrico Zio},
keywords = {Time series regression, Support vector machines, Particle filter, Reliability prediction},
abstract = {Support vector regression (SVR) has been applied to time series prediction and some works have demonstrated the feasibility of its use to forecast system reliability. For accuracy of reliability forecasting, the selection of SVR's parameters is important. The existing research works on SVR's parameters selection divide the example dataset into training and test subsets, and tune the parameters on the training data. However, these fixed parameters can lead to poor prediction capabilities if the data of the test subset differ significantly from those of training. Differently, the novel method proposed in this paper uses particle filtering to estimate the SVR model parameters according to the whole measurement sequence up to the last observation instance. By treating the SVR training model as the observation equation of a particle filter, our method allows updating the SVR model parameters dynamically when a new observation comes. Because of the adaptability of the parameters to dynamic data pattern, the new PF–SVR method has superior prediction performance over that of standard SVR. Four application results show that PF–SVR is more robust than SVR to the decrease of the number of training data and the change of initial SVR parameter values. Also, even if there are trends in the test data different from those in the training data, the method can capture the changes, correct the SVR parameters and obtain good predictions.}
}
@article{AKBARI2021102917,
title = {Schizophrenia recognition based on the phase space dynamic of EEG signals and graphical features},
journal = {Biomedical Signal Processing and Control},
volume = {69},
pages = {102917},
year = {2021},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2021.102917},
url = {https://www.sciencedirect.com/science/article/pii/S1746809421005140},
author = {Hesam Akbari and Sedigheh Ghofrani and Pejman Zakalvand and Muhammad {Tariq Sadiq}},
keywords = {Electroencephalography (EEG), Phase space dynamic, Schizophrenic, Graphical feature, Classification},
abstract = {Schizophrenia is a mental disorder that causes adverse effects on the mental capacity of a person, emotional inclinations, and quality of personal and social life. The official statistics reported that about 20 million people suffer from this severe mental illness worldwide. The manual screening of schizophrenic patients is tedious, time-consuming, costly, and prone to human error. Thus, it is necessary to provide a fully automatic, fairly accurate, and reasonably inexpensive system to diagnose schizophrenia patients. Electroencephalography (EEG) is commonly employed to evaluate and detect the brain's functions and disorders. The purpose of this study is to introduce, apply, and examine a novel framework to automatically diagnose schizophrenia disorders. This procedure is performed by using the phase space dynamic (PSD) of EEG signals. The two-dimensional PSD of EEG signals is first plotted on Cartesian space. Then, fifteen graphical features are extracted to evaluate the chaotic behavior of PSD based on healthy and schizophrenic subjects. Also, a sizeable number of remarkable features and optimum channels are obtained by the forward selection algorithm (FSA). Finally, eight different classifiers are tested for schizophrenia detection. In this case, the K-nearest neighbor (KNN) and generalized regression neural network (GRNN) showed better performance than the others. As a result, using a 10-fold cross-validation strategy, the KNN classifier with City-block distance reached the level of the average classification accuracy (ACC) of 94.80%, the sensitivity (SEN) of 94.30%, and the specificity (SPE) of 95.20%. The findings of the study confirmed that the PSD shape of the Cz channel for schizophrenia groups is more regular than the healthy ones. It can be applied as a biomarker for the medical team to diagnose schizophrenia disorder. It was found that the frontal and parietal lobes reflect the effects of schizophrenia disorder better than the other lobes. Consequently, the proposed framework contributes to realizing a real-time, easily accessible, and fairly inexpensive method in clinics and hospitals to quickly detect schizophrenia disorder.}
}
@article{CASTROGAMA2014250,
title = {Flood inference simulation using surrogate modelling for the Yellow River multiple reservoir system},
journal = {Environmental Modelling & Software},
volume = {55},
pages = {250-265},
year = {2014},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2014.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S1364815214000528},
author = {M.E. Castro-Gama and I. Popescu and S. Li and A. Mynett and A. {van Dam}},
keywords = {Reservoir operation, Yellow River, Flood modeling, Surrogate modeling},
abstract = {The Yellow River, in China, is one of the largest hydro systems in the world. Flooding is a major problem for the river, and therefore over the last 50 years a large number of interventions have been made in its reaches and tributaries, in order to control the flooding events in the lowland area, downstream of the Huayuankou hydrological station. The development of new technologies and approaches to decision support has raised possibilities for creating new ways of managing the river and reducing loss of life, in the case of flooding, for the people living within the embankment area of the river. Given the importance of the river for the development of economic activity in China, it is essential to increase the understanding of the general flooding processes triggered by several reservoir operation scenarios, and then, after applying them to a flooding model of a specific area, to test the findings. The main goal of the research presented here is to investigate and develop the statistical inference between the operation of reservoirs on the Yellow River and a set of variables related to the downstream flooding, such as the total flooding volume and the peak discharge. The research shows that it is possible to use such inference models as decision support tools, by reducing the number of explanatory variables to be included in the simulations carried out to determine the appropriate reservoir operation.}
}
@article{SHEN2021107431,
title = {Fast and robust identification of railway track stiffness from simple field measurement},
journal = {Mechanical Systems and Signal Processing},
volume = {152},
pages = {107431},
year = {2021},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2020.107431},
url = {https://www.sciencedirect.com/science/article/pii/S0888327020308177},
author = {Chen Shen and Rolf Dollevoet and Zili Li},
keywords = {Railway track stiffness, Structural identification, Frequency response function, Field hammer test, Gaussian process regression},
abstract = {We propose to combine a physics-based finite element (FE) track model and a data-driven Gaussian process regression (GPR) model to directly infer railpad and ballast stiffness from measured frequency response functions (FRF) by field hammer tests. Conventionally, only the rail resonance and full track resonance are used as the FRF features to identify track stiffness. In this paper, eleven features, including sleeper resonances, from a single FRF curve are selected as the predictors of the GPR. To deal with incomplete measurements and uncertainties in the FRF features, we train multiple candidate GPR models with different features, kernels and training sets. Predictions by the candidate models are fused using a weighted Product of Experts method that automatically filters out unreliable predictions. We compare the performance of the proposed method with a model updating method using the particle swam optimization (PSO) on two synthesis datasets in a wide range of scenarios. The results show that the enriched features and the proposed fusion strategy can effectively reduce prediction errors. In the worst-case scenario with only three features and 5% injected noise, the average prediction errors for the railpad and ballast stiffness are approximately 12% and 6%, outperforming the PSO by about 6% and 3%, respectively. Moreover, the method enables fast predictions for large datasets. The predictions for 400 samples takes only approximately 10 s compared with 40 min using the PSO. Finally, a field application example shows that the proposed method is capable of extracting the stiffness values using a simple setup, i.e., with only one accelerometer and one impact location.}
}
@article{NAM2021,
title = {Discovery of Depression-Associated Factors From a Nationwide Population-Based Survey: Epidemiological Study Using Machine Learning and Network Analysis},
journal = {Journal of Medical Internet Research},
volume = {23},
number = {6},
year = {2021},
issn = {1438-8871},
doi = {https://doi.org/10.2196/27344},
url = {https://www.sciencedirect.com/science/article/pii/S1438887121005811},
author = {Sang Min Nam and Thomas A Peterson and Kyoung Yul Seo and Hyun Wook Han and Jee In Kang},
keywords = {depression, epidemiology, machine learning, network, prediction model, XGBoost},
abstract = {Background
In epidemiological studies, finding the best subset of factors is challenging when the number of explanatory variables is large.
Objective
Our study had two aims. First, we aimed to identify essential depression-associated factors using the extreme gradient boosting (XGBoost) machine learning algorithm from big survey data (the Korea National Health and Nutrition Examination Survey, 2012-2016). Second, we aimed to achieve a comprehensive understanding of multifactorial features in depression using network analysis.
Methods
An XGBoost model was trained and tested to classify “current depression” and “no lifetime depression” for a data set of 120 variables for 12,596 cases. The optimal XGBoost hyperparameters were set by an automated machine learning tool (TPOT), and a high-performance sparse model was obtained by feature selection using the feature importance value of XGBoost. We performed statistical tests on the model and nonmodel factors using survey-weighted multiple logistic regression and drew a correlation network among factors. We also adopted statistical tests for the confounder or interaction effect of selected risk factors when it was suspected on the network.
Results
The XGBoost-derived depression model consisted of 18 factors with an area under the weighted receiver operating characteristic curve of 0.86. Two nonmodel factors could be found using the model factors, and the factors were classified into direct (P<.05) and indirect (P≥.05), according to the statistical significance of the association with depression. Perceived stress and asthma were the most remarkable risk factors, and urine specific gravity was a novel protective factor. The depression-factor network showed clusters of socioeconomic status and quality of life factors and suggested that educational level and sex might be predisposing factors. Indirect factors (eg, diabetes, hypercholesterolemia, and smoking) were involved in confounding or interaction effects of direct factors. Triglyceride level was a confounder of hypercholesterolemia and diabetes, smoking had a significant risk in females, and weight gain was associated with depression involving diabetes.
Conclusions
XGBoost and network analysis were useful to discover depression-related factors and their relationships and can be applied to epidemiological studies using big survey data.}
}
@article{ARGHA2018,
title = {Effect of Seasonal Variation on Clinical Outcome in Patients with Chronic Conditions: Analysis of the Commonwealth Scientific and Industrial Research Organization (CSIRO) National Telehealth Trial},
journal = {JMIR Medical Informatics},
volume = {6},
number = {1},
year = {2018},
issn = {2291-9694},
doi = {https://doi.org/10.2196/medinform.9680},
url = {https://www.sciencedirect.com/science/article/pii/S2291969418000133},
author = {Ahmadreza Argha and Andrey Savkin and Siaw-Teng Liaw and Branko George Celler},
keywords = {telehealth, telemonitoring, seasonal variation, clinical trial, vital signs, chronic disease},
abstract = {Background
Seasonal variation has an impact on the hospitalization rate of patients with a range of cardiovascular diseases, including myocardial infarction and angina. This paper presents findings on the influence of seasonal variation on the results of a recently completed national trial of home telemonitoring of patients with chronic conditions, carried out at five locations along the east coast of Australia.
Objective
The aim is to evaluate the effect of the seasonal timing of hospital admission and length of stay on clinical outcome of a home telemonitoring trial involving patients (age: mean 72.2, SD 9.4 years) with chronic conditions (chronic obstructive pulmonary disease coronary artery disease, hypertensive diseases, congestive heart failure, diabetes, or asthma) and to explore methods of minimizing the influence of seasonal variations in the analysis of the effect of at-home telemonitoring on the number of hospital admissions and length of stay (LOS).
Methods
Patients were selected from a hospital list of eligible patients living with a range of chronic conditions. Each test patient was case matched with at least one control patient. A total of 114 test patients and 173 control patients were available in this trial. However, of the 287 patients, we only considered patients who had one or more admissions in the years from 2010 to 2012. Three different groups were analyzed separately because of substantially different climates: (1) Queensland, (2) Australian Capital Territory and Victoria, and (3) Tasmania. Time series data were analyzed using linear regression for a period of 3 years before the intervention to obtain an average seasonal variation pattern. A novel method that can reduce the impact of seasonal variation on the rate of hospitalization and LOS was used in the analysis of the outcome variables of the at-home telemonitoring trial.
Results
Test patients were monitored for a mean 481 (SD 77) days with 87% (53/61) of patients monitored for more than 12 months. Trends in seasonal variations were obtained from 3 years’ of hospitalization data before intervention for the Queensland, Tasmania, and Australian Capital Territory and Victoria subgroups, respectively. The maximum deviation from baseline trends for LOS was 101.7% (SD 42.2%), 60.6% (SD 36.4%), and 158.3% (SD 68.1%). However, by synchronizing outcomes to the start date of intervention, the impact of seasonal variations was minimized to a maximum of 9.5% (SD 7.7%), thus improving the accuracy of the clinical outcomes reported.
Conclusions
Seasonal variations have a significant effect on the rate of hospital admission and LOS in patients with chronic conditions. However, the impact of seasonal variation on clinical outcomes (rate of admissions, number of hospital admissions, and LOS) of at-home telemonitoring can be attenuated by synchronizing the analysis of outcomes to the commencement dates for the telemonitoring of vital signs.
Trial Registration
Australian New Zealand Clinical Trial Registry ACTRN12613000635763; https://www.anzctr.org.au/Trial/Registration/TrialReview.aspx?id=364030&isReview=true (Archived by WebCite at http://www.webcitation.org/ 6xLPv9QDb)}
}
@article{SCHMIDT2021103963,
title = {Bayesian hierarchical and measurement uncertainty model building for liquefaction triggering assessment},
journal = {Computers and Geotechnics},
volume = {132},
pages = {103963},
year = {2021},
issn = {0266-352X},
doi = {https://doi.org/10.1016/j.compgeo.2020.103963},
url = {https://www.sciencedirect.com/science/article/pii/S0266352X20305267},
author = {Jonathan Schmidt and Robb Moss},
keywords = {Liquefaction, Bayesian statistics, Predictive modeling, Earthquake engineering, Natural hazards},
abstract = {This study examines the details of creating and validating an empirical liquefaction model, using a worldwide cone penetration test (CPT) liquefaction database with the intent of incorporating the rigor found in predictive modeling in other fields and addressing shortcomings of existing models. Our study implements a logistic regression within a Bayesian measurement error framework to incorporate uncertainty in predictor variables and allow for a probabilistic interpretation of model parameters when making future predictions. The model is built using a hierarchal approach to account for intra-event correlation in loading variables and differences in event sample sizes. The model is tested using an independent set of recent case histories. We found that the Bayesian measurement error model considering two predictor variables, normalized CPT tip resistance and cyclic stress ratio decreased model uncertainty while maintaining predictive utility for new data. Hierarchical models revealed high model uncertainty potentially due to the database lacking in high loading non-liquefaction sites. Models considering friction ratio as a predictor variable performed worse than the two variable case and will require more data or informative priors to be adequately estimated. The framework developed is flexible and can be extended using different methods of predictor variable selection, model function forms, and validation processes.}
}
@article{GONG2020101455,
title = {Heat load prediction of residential buildings based on discrete wavelet transform and tree-based ensemble learning},
journal = {Journal of Building Engineering},
volume = {32},
pages = {101455},
year = {2020},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2020.101455},
url = {https://www.sciencedirect.com/science/article/pii/S2352710219324799},
author = {Mingju Gong and Jin Wang and Yin Bai and Bo Li and Lei Zhang},
keywords = {District heating system, Load prediction, Feature selection, Discrete wavelet transform, Ensemble learning},
abstract = {Currently, the high energy consumption of district heating system represents a problem. Thus, accurate prediction of future heat load is the key to ensure energy saving and improve system operation efficiency. However, the accuracy of the prediction model based on single machine learning algorithms need to be improved, and the hybrid prediction model suffers from redundant inputs. The objective of this study was to obtain the most accurate prediction of the short-term heat load consumption. The discrete wavelet transform, two tree-based ensemble learning algorithms, extremely randomized trees regression and gradient boosting decision tree were used to establish hybrid models for predicting the heat load 1 h in advance. Pearson and least absolute shrinkage and selection operator (LASSO) methods were used to optimize the feature set, including system parameters, meteorological parameters and time steps. This resulted in four feature sets. In addition, the effect of the historical heat load on the performance of the prediction model was analysed. Actual operation data of an established heating system were collected to test the performance of the models. The results were then compared with those of the models established by artificial neural network and support vector regression. The results show that the historical heat load had a significant impact on the prediction accuracy of the heat load of the upcoming hour, and the extreme random tree model based on discrete wavelet transform showed a better prediction performance and lower model complexity in case of using the feature set selected by the LASSO method.}
}
@article{JACKEL2020101817,
title = {Design of an aeronautic pitot probe with a redundant heating system incorporating phase change materials},
journal = {Flow Measurement and Instrumentation},
volume = {76},
pages = {101817},
year = {2020},
issn = {0955-5986},
doi = {https://doi.org/10.1016/j.flowmeasinst.2020.101817},
url = {https://www.sciencedirect.com/science/article/pii/S095559862030159X},
author = {Robert Jäckel and Fidencio Tapia and Geydy Gutiérrez-Urueta and Cintia {Monreal Jiménez}},
keywords = {Aeronautical pitot probe, Phase change materials, Design of experiment method, Heating element, Aircraft icing},
abstract = {The aim of this work is the design of a pitot probe (PP) prototype in order to retard the cool down of the tip, in case of a heating element failure. The viability of operation in flight conditions is evaluated. The design consists of a redundant heating system incorporating phase change materials (PCM). Combining experimental observations of ice formation with the implementation of the conjugate heat transfer (CHT) model, with the addition of the heat release due to the phase change of the PCM, the numerical evaluation is developed. The modelling assumptions and numerical implementation of the phase change process are presented. Then, the selection an appropriate PCM is based on the low flammability and volume dilation and the quantitative effects of the material properties on the heat transfer. A commercial PCM solution based on salt hydrates was chosen as the most adequate for the design. The parametric design of the prototype, based on the design of experiment method and fractional factorial testing, is established. A multiple linear regression model was obtained in order to maximize the cooling retardation. The numerical simulations demonstrate that the prototype PP tip temperature remains 194 s longer above 0 °C than that of the conventional model analyzed.}
}
@article{CHANG2023,
title = {Comparing Natural Language Processing and Structured Medical Data to Develop a Computable Phenotype for Patients Hospitalized Due to COVID-19: Retrospective Analysis},
journal = {JMIR Medical Informatics},
volume = {11},
year = {2023},
issn = {2291-9694},
doi = {https://doi.org/10.2196/46267},
url = {https://www.sciencedirect.com/science/article/pii/S2291969423000182},
author = {Feier Chang and Jay Krishnan and Jillian H Hurst and Michael E Yarrington and Deverick J Anderson and Emily C O'Brien and Benjamin A Goldstein},
keywords = {natural language processing, NLP, computable phenotype, machine learning, COVID, coronavirus, hospitalize, hospitalization, electronic health record, EHR, health record, structured data, data element, free text, unstructured data, provider note, classify, classification, algorithm, COVID-19},
abstract = {Background
Throughout the COVID-19 pandemic, many hospitals conducted routine testing of hospitalized patients for SARS-CoV-2 infection upon admission. Some of these patients are admitted for reasons unrelated to COVID-19 and incidentally test positive for the virus. Because COVID-19–related hospitalizations have become a critical public health indicator, it is important to identify patients who are hospitalized because of COVID-19 as opposed to those who are admitted for other indications.
Objective
We compared the performance of different computable phenotype definitions for COVID-19 hospitalizations that use different types of data from electronic health records (EHRs), including structured EHR data elements, clinical notes, or a combination of both data types.
Methods
We conducted a retrospective data analysis, using clinician chart review–based validation at a large academic medical center. We reviewed and analyzed the charts of 586 hospitalized individuals who tested positive for SARS-CoV-2 in January 2022. We used LASSO (least absolute shrinkage and selection operator) regression and random forests to fit classification algorithms that incorporated structured EHR data elements, clinical notes, or a combination of structured data and clinical notes. We used natural language processing to incorporate data from clinical notes. The performance of each model was evaluated based on the area under the receiver operator characteristic curve (AUROC) and an associated decision rule based on sensitivity and positive predictive value. We also identified top words and clinical indicators of COVID-19–specific hospitalization and assessed the impact of different phenotyping strategies on estimated hospital outcome metrics.
Results
Based on a chart review, 38.2% (224/586) of patients were determined to have been hospitalized for reasons other than COVID-19, despite having tested positive for SARS-CoV-2. A computable phenotype that used clinical notes had significantly better discrimination than one that used structured EHR data elements (AUROC: 0.894 vs 0.841; P<.001) and performed similarly to a model that combined clinical notes with structured data elements (AUROC: 0.894 vs 0.893; P=.91). Assessments of hospital outcome metrics significantly differed based on whether the population included all hospitalized patients who tested positive for SARS-CoV-2 or those who were determined to have been hospitalized due to COVID-19.
Conclusions
These findings highlight the importance of cause-specific phenotyping for COVID-19 hospitalizations. More generally, this work demonstrates the utility of natural language processing approaches for deriving information related to patient hospitalizations in cases where there may be multiple conditions that could serve as the primary indication for hospitalization.}
}
@article{CASTILLOBARNES2020153,
title = {Autosomal dominantly inherited alzheimer disease: Analysis of genetic subgroups by machine learning},
journal = {Information Fusion},
volume = {58},
pages = {153-167},
year = {2020},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2020.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S1566253519309959},
author = {Diego Castillo-Barnes and Li Su and Javier Ramírez and Diego Salas-Gonzalez and Francisco J. Martinez-Murcia and Ignacio A. Illan and Fermin Segovia and Andres Ortiz and Carlos Cruchaga and Martin R. Farlow and Chengjie Xiong and Neil R. Graff-Radford and Peter R. Schofield and Colin L. Masters and Stephen Salloway and Mathias Jucker and Hiroshi Mori and Johannes Levin and Juan M. Gorriz and Dominantly Inherited Alzheimer Network (DIAN)},
keywords = {Dominantly-inherited Alzheimer’s disease (DIAD), DIAN, Alzheimer’s disease (AD), Neuroimaging, Machine learning},
abstract = {Despite subjects with Dominantly-Inherited Alzheimer’s Disease (DIAD) represent less than 1% of all Alzheimer’s Disease (AD) cases, the Dominantly Inherited Alzheimer Network (DIAN) initiative constitutes a strong impact in the understanding of AD disease course with special emphasis on the presyptomatic disease phase. Until now, the 3 genes involved in DIAD pathogenesis (PSEN1, PSEN2 and APP) have been commonly merged into one group (Mutation Carriers, MC) and studied using conventional statistical analysis. Comparisons between groups using null-hypothesis testing or longitudinal regression procedures, such as the linear-mixed-effects models, have been assessed in the extant literature. Within this context, the work presented here performs a comparison between different groups of subjects by considering the 3 genes, either jointly or separately, and using tools based on Machine Learning (ML). This involves a feature selection step which makes use of ANOVA followed by Principal Component Analysis (PCA) to determine which features would be realiable for further comparison purposes. Then, the selected predictors are classified using a Support-Vector-Machine (SVM) in a nested k-Fold cross-validation resulting in maximum classification rates of 72–74% using PiB PET features, specially when comparing asymptomatic Non-Carriers (NC) subjects with asymptomatic PSEN1 Mutation-Carriers (PSEN1-MC). Results obtained from these experiments led to the idea that PSEN1-MC might be considered as a mixture of two different subgroups including: a first group whose patterns were very close to NC subjects, and a second group much more different in terms of imaging patterns. Thus, using a k-Means clustering algorithm it was determined both subgroups and a new classification scenario was conducted to validate this process. The comparison between each subgroup vs. NC subjects resulted in classification rates around 80% underscoring the importance of considering DIAN as an heterogeneous entity.}
}
@article{RAO2017103,
title = {A multi-objective algorithm for optimization of modern machining processes},
journal = {Engineering Applications of Artificial Intelligence},
volume = {61},
pages = {103-125},
year = {2017},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2017.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0952197617300465},
author = {R. Venkata Rao and Dhiraj P. Rai and Joze Balic},
keywords = {Modern machining processes, Process parameters, Multi-objective optimization, A posteriori approach, Jaya algorithm, Population based algorithms},
abstract = {Multi-objective optimization aspects of four modern machining processes namely wire-electro discharge machining process, laser cutting process, electrochemical machining process and focused ion beam micro-milling process are considered in this work. In WEDM process cutting velocity and surface quality are important objectives which are mutually conflicting in nature. Minimization of kerf taper is vital in the laser cutting process which increases with the increase in material removal rate. The ECM process is characterized by high material removal rate, but poor dimensional accuracy, high tool wear rate and high over cut. FIB micro-milling process is useful in applications where a nano-level surface finish is desired but this process is characterized by a very low material removal rate. All the above mentioned objectives are vital as they closely govern the performance of the machining processes considered in this work. Therefore, the aim of this work is to achieve these objectives through process parameter optimization. In order to handle multiple objectives simultaneously a new posteriori multi-objective optimization algorithm named as multi-objective Jaya (MO-Jaya) algorithm is proposed which can provide multiple optimal solutions in a single simulation run. The regression models for the above mentioned machining processes which were developed by previous researchers are used as fitness function for MO-Jaya algorithm. In the case of WEDM process the optimization problem is an unconstrained, linear and parameter bounded. In the case of laser cutting process the optimization problem is a non-linear, unconstrained, quadratic and parameter bounded. In the ECM process the optimization problem is a non-linear, unconstrained, quadratic and parameter bounded. The second case study of ECM process the optimization problem is a non-linear, constrained, non-quadratic and parameter bounded. In the case of FIB micro-milling process, the optimization problem is a non-linear, unconstrained, quadratic and parameter bounded. In addition, the performance of MO-Jaya algorithm is also tested on a non-linear, non-quadratic unconstrained multi-objective benchmark function of CEC2009. In order to handle the constraints effectively a heuristic approach for handling constraints known as the constrained-dominance concept is used in MO-Jaya algorithm. In order to ensure that the newly generated solutions are within the parameter bounds a parameter-bounding strategy is used in MO-Jaya algorithm. The results of MO-Jaya algorithm are compared with the results of GA, NSGA, NSGA-II, BBO, NSTLBO, PSO, SQP and Monte Carlo simulations. The results have shown the better performance of the proposed algorithm.}
}
@article{VESELY201620,
title = {Multi-parameter approximation of the stress field in a cracked body in the more distant surroundings of the crack tip},
journal = {International Journal of Fatigue},
volume = {89},
pages = {20-35},
year = {2016},
note = {Special Issue: Crack Tip Fields 3},
issn = {0142-1123},
doi = {https://doi.org/10.1016/j.ijfatigue.2016.02.016},
url = {https://www.sciencedirect.com/science/article/pii/S014211231600061X},
author = {Václav Veselý and Jakub Sobek and Petr Frantík and Stanislav Seitl},
keywords = {Crack-tip fields, Williams power series, Higher order terms, Stress field reconstruction, Multi-parameter approximation accuracy},
abstract = {The accuracy of the multi-parameter approximation of the stress fields in a cracked body is studied in the paper. This approximation, which uses the Williams power series expansion (WE), is intended to be used to estimate the extent of the nonlinear zone, which plays a significant role within tensile failure and the fatigue assessment of non-brittle materials. The characteristics of this zone could be potentially incorporated into methods of determining the true values of fracture parameters and the fatigue behaviour descriptors of materials exhibiting nonlinear failure. Considering the fact that in the case of elastic–plastic and especially quasi-brittle materials the size of this zone is substantial in comparison to specimen dimensions, it is necessary to consider a large region around the crack tip for this task. An automatic routine implemented as a Java application was created to determine the values of coefficients of the higher order terms of the WE that describe crack-tip fields. These values are calculated using the Over-Deterministic Method (ODM), which is applied to the results of the finite element (FE) analysis of an arbitrary mode I test geometry. Furthermore, another Java application developed by the authors provides an analytical reconstruction of the crack-tip stress field by means of the truncated WE, and enables detailed analysis of the crack-tip stress field approximation. The developed procedures simplify the analysis of the description of mechanical fields at a greater distance from the crack tip considerably. The presented research is focused on the optimisation of the selection of FE nodal results entering the ODM procedure used to determine the values of coefficients of the higher order terms of the WE. The aim is to improve the accuracy of the approximation.}
}
@article{HONG201626,
title = {A spatially autoregressive and heteroskedastic space-time pedestrian exposure modeling framework with spatial lags and endogenous network topologies},
journal = {Analytic Methods in Accident Research},
volume = {10},
pages = {26-46},
year = {2016},
issn = {2213-6657},
doi = {https://doi.org/10.1016/j.amar.2016.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S2213665716300112},
author = {Jungyeol Hong and Venky N. Shankar and Narayan Venkataraman},
keywords = {Pedestrian exposure, Network topology, Spatial autoregressive, GMM estimation},
abstract = {The main objective of this study is to derive a modeling framework for characterizing the space-time exposure of pedestrians in crosswalks, where the spatial measure is characterized by pedestrian density and the temporal measure is characterized by crosswalk time occupancy. This characterization has not been observed in the literature, but is a characterization that allows one to differentiate the components of pedestrian exposure with enhanced resolution in space and time. However, real-time observations to generate space-time data are time consuming and expensive over a large urban network. A hybrid microsimulation-statistical approach is utilized for data generation and statistical analysis in this study. The exposure models predicting crosswalk density and occupancy were estimated using spatial autoregressive models with spatial lags, autoregressive and heteroskedastic spatial disturbances and endogenous regressors. An instrumental variables generalized method of moments (IV-GMM) approach was used for estimation, and the spatial models account for spatial dependence among crosswalks through the estimation of spatial lag and spatial correlation parameters. In a case study of the downtown crosswalk grid in Seattle, Washington, 688 crosswalks were modeled using ten network topology measures capturing node degree, centrality, clustering, modularity, attractiveness and eccentricity measures. The models utilized these network topology variables to account for stochasticity in network design effects on pedestrian dynamics. Several important findings resulted from this study. First, and most important, it was found that network topology measures had an endogenous impact on pedestrian density. Second, the pedestrian time occupancy equation is characterized by endogenous selection effects. That is, in crosswalks with persistent pedestrian volumes and positive densities, the impact of pedestrian trip generation volumes and pedestrian density were corrected for endogeneity and selection bias. The combined results of the pedestrian density and time occupancy equations indicate that endogeneity and selection bias are critical issues that should not be ignored in pedestrian exposure modeling. Pedestrian trip generation volumes representing block level facility generation were found to be elastic. This finding indicates the utility of our modeling framework for estimating the impact of land use on pedestrian space-time exposure at the block level. Out-of-sample prediction tests of the density and time occupancy models and comparisons with pedestrian count data from field observations indicated substantial predictive accuracies. Finally, it was determined that degree and hub were highly sensitive network design parameters in terms of their influence on density. The average total impact (marginal effect) of these measures indicates that attention should be paid to crosswalk network design from the standpoint of degree and hub characteristics. These results show that our space-time density-occupancy modeling framework is a plausible and efficient predictive tool that can be used to estimate pedestrian crosswalk exposure using building level and network topology data alone. We find that the IV-GMM technique is a useful approach for the emergent problem of inference in hybrid simulation-statistical transportation datasets, due to fewer assumptions on distributional assumptions about the data, while accounting for statistical effects relating to endogeneity, potential selection effects and heteroscedasticity.}
}
@article{SHEN2021101283,
title = {Many-to-many comprehensive relative importance analysis and its applications to analysis of semiconductor electrical testing parameters},
journal = {Advanced Engineering Informatics},
volume = {48},
pages = {101283},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101283},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621000379},
author = {Zixin Shen and Amos Hong and Argon Chen},
keywords = {Canonical correlation analysis, Semiconductor yield analysis, Feature selection, Multivariate analysis, Relative importance, Relative weight},
abstract = {Most engineering systems have multiple inputs and multiple outputs. For example, a semiconductor manufacturing system consists of thousands of fabrication steps with numerous inline production parameters affecting multiple electrical characteristics of final chips. Many-to-many analysis is thus needed to more effectively discover critical factors causing poor product qualities or a low production yield. Though methodologies of many-to-many correlation analysis have been proposed in the literature, difficulties arise, especially when there exist multicollinearity effects among features, to measure the relative importance of a feature’s contribution. Relative weight analysis offers a general framework for determining the relative importance of features in multiple linear regression models. In this article, we propose a many-to-many comprehensive relative importance analysis based on canonical correlation analysis to effectively summarize the relationship between two sets of features. Simulation and actual semiconductor yield-analysis cases are used to show the proposed method, as compared to other conventional methods, in analysis of two sets of features.}
}
@article{MADARY2021259,
title = {A Bayesian Framework for Large-Scale Identification of Nonlinear Hybrid Systems},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {5},
pages = {259-264},
year = {2021},
note = {7th IFAC Conference on Analysis and Design of Hybrid Systems ADHS 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.508},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321012830},
author = {Ahmad Madary and Hamid Reza Momeni and Alessandro Abate and Kim G. Larsen},
keywords = {Nonlinear hybrid systems, Switched nonlinear ARX models, Bayesian inference, System identification, Occam’s Razor principle, Large data sets},
abstract = {In this paper, a two-level Bayesian framework is proposed for the identification of nonlinear hybrid systems from large data sets by embedding it in a four-stage procedure. At the first stage, feature vector selection techniques are used to generate a reduced-size set from the given training data set. The resulting data set then is used to identify the hybrid system using a Bayesian method, where the objective is to assign each data point to a corresponding sub-mode of the hybrid model. At the third stage, this data assignment is used to train a Bayesian classifier to separate the original data set and determine the corresponding sub-mode for all the original data points. Finally, once every data point is assigned to a sub-mode, a Bayesian estimator is used to estimate a regressor for each sub-system independently. The proposed method tested on three case studies.}
}
@article{LI2016183,
title = {Bayesian hazard modeling based on lifetime data with latent heterogeneity},
journal = {Reliability Engineering & System Safety},
volume = {145},
pages = {183-189},
year = {2016},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2015.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S0951832015002719},
author = {Mingyang Li and Jian Liu},
keywords = {Bayesian inference, Mixture model, Hazard regression, Gibbs sampler, Model selection},
abstract = {Lifetime data collected from reliability tests or field operations often exhibit significant heterogeneity patterns caused by latent factors. Such latent heterogeneity indicates that lifetime observations may belong to different sub-populations with different distribution parameters. As a result, the assumption on data homogeneity adopted by conventional reliability modeling techniques becomes inappropriate. Effective identification and quantification of such heterogeneity is crucial for more reliable model estimation and subsequent optimal decision making in a variety of reliability assurance activities. This research proposes a full Bayesian modeling framework for statistical hazard modeling of latent heterogeneity in lifetime data. The proposed framework is generic and comprehensive by systematically addressing different modeling aspects, which include modeling sub-populations with different hazard rates changing over time and different responses to the same stress factors, determining the number of sub-populations, identifying the most appropriate sub-population model structures, estimating model parameters and performing predictive inference. A numerical case study demonstrates the validity and effectiveness of the proposed approach.}
}
@article{OLALUSI2021111470,
title = {Shear capacity prediction of slender reinforced concrete structures with steel fibers using machine learning},
journal = {Engineering Structures},
volume = {227},
pages = {111470},
year = {2021},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2020.111470},
url = {https://www.sciencedirect.com/science/article/pii/S0141029620340712},
author = {Oladimeji Benedict Olalusi and Paul O. Awoyera},
keywords = {Steel fiber reinforced concrete beam, Shear capacity, Structural design, Model uncertainty, Partial safety factors, Structural reliability, Artificial intelligence},
abstract = {Shear failure in reinforced concrete beams poses a critical safety issue since it may occur without any prior signs of damage in some cases. Many of the existing shear design equations for steel fiber reinforced concrete (SFRC) beams include significant uncertainty due to failure in reflecting the phenomenology of shear resistance accurately. Given these, adequate reliability evaluation of shear design provisions for SFRC beam is of high significance, and increased accuracy and minimisation of variability in the predictive model is essential. This contribution proposes machine learning (ML) based methods - Gaussian Process regression (GPR) and the Random Forest (RF) techniques - to predict the ultimate shear resistance of SFRC slender beams without stirrups. The models were developed using a database of 326 experimental SFRC slender beams obtained from previous studies, utilising 75% for model training and the remainder for testing. The performance of the proposed models was assessed by statistical comparison to experimental results and to that of the state-of-practice existing shear design models (fib Model Code 2010, German guideline, Bernat et al. model). The proposed ML-based models are in close alignment with the experimentally observed shear strength and the existing predictive models, but provide more accurate and unbiased predictions. Furthermore, the model uncertainty of the various resistance models was characterised and investigated. The ML-based models displayed the lowest bias and variability, with no significant trend with input parameters. The inconsistencies observed in the predictions by the existing shear design formulations at the variation of shear span to effective depth ratio is a major cause for concern; reliability analysis is required. Finally, partial resistance safety factors were proposed for the model uncertainty associated with the existing shear design equations.}
}
@article{EVORA2017116,
title = {Neural network models for supporting drug and multidrug resistant tuberculosis screening diagnosis},
journal = {Neurocomputing},
volume = {265},
pages = {116-126},
year = {2017},
note = {New Trends for Pattern Recognition: Theory & Applications},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2016.08.151},
url = {https://www.sciencedirect.com/science/article/pii/S0925231217310160},
author = {L.H.R.A. Évora and J.M. Seixas and A.L. Kritski},
keywords = {Tuberculosis, Multidrug-resistant tuberculosis, Drug-resistant tuberculosis, Drug-sensitive tuberculosis, Neural networks, Clinical score},
abstract = {Tuberculosis (TB) is the leading cause of global mortality among communicable diseases. The diagnosis of Drug-Resistant Tuberculosis (DR-TB) demands even more attention, leading to longer treatments and higher deceased rates. All diagnostic methods available have deficiencies in their detection rates, time release results, or have a higher cost and need a complex infrastructure to setup. New molecular diagnostics, such as the Xpert MTB/RIF assay, have great potential for revolutionizing the diagnosis of Rifampicin Resistance (RR). But, a positive RR result with this test should be carefully interpreted and take into consideration the risk of Multidrug-Resistant TB (MDR-TB) according to its prevalence, locally. Therefore, the development of screening approaches for DR/MDR-TB suspects would help to identify those should be tested by Xpert MTB/RIF. This work develops Artificial Neural Network (ANN) models considering data from presumed DR/MDR-TB subjects according to the National Guidelines at Rio de Janeiro/Brazil, attended in reference centers in Rio de Janeiro, from Feb 2011 and May 2013. Subjects aged 18 years or older, and results were compared with models based on Classification And Regression Trees (CART). Practical operation at different epidemiological scenarios are considered by constructing models using different variable selection criteria, so that environments with low resource conditions can be assisted. Among 280 presumed DR-TB cases included, 38 were DR-TB, 48-MDR, 32-Drug-Sensitive and 162 with no TB. Between DR-TB and non DR-TB, the sensitivity and specificity reached 95.1%(±5.0) and 85.0%(±4.9), respectively. The promising results of clinical score for DR/MDR-TB diagnosis indicate that this approach may be used in the evaluation of presumed DR/MDR-TB.}
}
@article{SIRCAR2021112308,
title = {Study and characterization of potential adsorbent materials for the design of the hydrogen isotopes extraction and analysis system},
journal = {Fusion Engineering and Design},
volume = {166},
pages = {112308},
year = {2021},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2021.112308},
url = {https://www.sciencedirect.com/science/article/pii/S0920379621000843},
author = {Amit Sircar and V. Gayathri Devi and Deepak Yadav and Jyoti Shankar Mishra and Ranjana Gangradey and Gayathry J. and Rahul Tomar and Pragnesh B. Dhorajiya and Purvi Dave},
keywords = {Fuel cycle, Extraction, Adsorbent, Isotherm, Zeolites, Enthalpy of adsorption},
abstract = {One of the most challenging tasks in the design of the inner fuel cycle system lies in the effective design of Tritium Extraction System (TES), which involves proper extraction and purification of tritium in a fusion reactor. A prototype Hydrogen Isotopes Recovery System (HIRS) is being developed to validate the concepts for tritium extraction by adsorption mass transfer mechanism. The two main systems of HIRS are Atmospheric Molecular Sieve Bed (AMSB) adsorber and Cryogenic Molecular Sieve Bed (CMSB) adsorber. AMSB removes ppm levels of water vapour while CMSB removes ppm levels of hydrogen isotopes, oxygen and nitrogen gas from Helium purge gas. Selection of appropriate adsorbents for the HIRS is important for its efficient functioning. Adsorbents, namely, Zeolites 3A, 4A, 5A, 13X and Activated Carbon have been studied in detail at cryogenic temperatures to understand their surface characteristics and adsorption potential. In this work, we have generated the adsorption isotherms for hydrogen isotopes on potential adsorbents using a volumetric adsorption apparatus for a wide range of pressure from 1 Pa to 105 Pa. The BET and DFT models are applied for the determination of the textural properties of the adsorbents and the Langmuir-Freundlich composite model is used to fit the isotherm data. The parameters of the model were determined using nonlinear regression analysis. The enthalpy of adsorption is determined and is in the range of 8–11.5 kJ/mol for the adsorbate loadings corresponding to the partial pressure of hydrogen isotopes of ∼ 50–500 Pa in the TES. At low pressures of hydrogen isotopes, the free energy plot followed Henry’s law. The isotherms and enthalpy of adsorption clearly indicate reversible physisorption and monolayer formation of hydrogen isotopes on the potential adsorbents. Amongst hydrogen and deuterium, hydrogen is more strongly adsorbed on Zeolites 4A, whereas deuterium is more strongly adsorbed on Zeolites 13X. In case of activated carbon, no isotopic selectivity was observed. This study facilitates the selection of potential adsorbents for the CMSB and quantification of the parameters associated with the adsorbents.}
}
@article{SALCEDOSANZ201879,
title = {An efficient neuro-evolutionary hybrid modelling mechanism for the estimation of daily global solar radiation in the Sunshine State of Australia},
journal = {Applied Energy},
volume = {209},
pages = {79-94},
year = {2018},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2017.10.076},
url = {https://www.sciencedirect.com/science/article/pii/S0306261917314976},
author = {Sancho Salcedo-Sanz and Ravinesh C. Deo and Laura Cornejo-Bueno and Carlos Camacho-Gómez and Sujan Ghimire},
keywords = {Solar radiation estimation, Coral Reefs Optimization, Extreme Learning Machines, Grouping Genetic Algorithms, Hybrid modelling system},
abstract = {This research paper aims to develop a hybrid neuro-evolutionary wrapper-based model for daily global solar radiation estimation in the solar-rich Sunshine State of Queensland, Australia. To design a robust hybrid modelling mechanism, the Interim-ERA European Centre for Medium Range Weather Forecasting (ECMWF) Reanalysis data are employed to train and cross-validate the estimation model that is formulated by an evolutionary-type algorithm: the Coral Reefs Optimization (CRO) integrated with an Extreme Learning Machine (ELM) model. The hybrid CRO-(ELM) algorithm is applied in two stages: first for the feature selection process guided by an ELM algorithm (a class of fast training neural network tool) as the model’s fitness function to screen an optimal set of predictor variables and second, for the estimation of the solar radiation using the optimally screened variables by the final hybrid CRO-(ELM)-ELM system. To benchmark the performance of the hybrid CRO-ELM algorithm for this estimation problem we apply an alternative, yet a similar feature screening approach: the Grouping Genetic Algorithm encoded into the ELM-based model (GGA-(ELM) used as the predictor mechanism). After the feature selection process is performed by the CRO algorithm that extracts the data patterns for accurate estimation the alternative objective algorithm is applied (in this case the ELM again) to formulate the hybrid CRO-(ELM)-ELM modelling system. To provide a rigorous evaluation of the CRO-(ELM)-ELM hybrid system, alternative estimation approaches are considered: the Multivariate Adaptive Regression Splines (MARS), Multiple Linear Regression (MLR) and the Support Vector Regression (SVR). The hybrid CRO-(ELM)-ELM system is tested in a real problem where the results are evaluated by means of several statistical score metrics and diagnostic plots of the observed and the estimated daily global solar radiation in the testing phase. We show that the hybrid CRO-(ELM)-ELM model is able to yield promising results; thus improving those attained by the 7 alternative models (i.e., hybrid CRO-(ELM)-MARS, CRO-(ELM)-MLR and CRO-(ELM)-SVR and the GGA equivalent models). The study ascertains that the CRO-based hybrid system where a large pool of predictor data are carefully screened through a wrapper-based modelling system and the ELM model is applied as a objective estimation tool can be adopted as a qualified stratagem in solar radiation estimation problems. The proposed hybrid CRO-(ELM)-ELM system exhibits clear advantages over the alternative machine learning approaches tested and also over the other machine learning algorithms used without the feature selection tool; thus advocating its scientific utility in renewable energy applications.}
}
@article{HE2022123148,
title = {Application of deep-learning method in the conjugate heat transfer optimization of full-coverage film cooling on turbine vanes},
journal = {International Journal of Heat and Mass Transfer},
volume = {195},
pages = {123148},
year = {2022},
issn = {0017-9310},
doi = {https://doi.org/10.1016/j.ijheatmasstransfer.2022.123148},
url = {https://www.sciencedirect.com/science/article/pii/S0017931022006196},
author = {Qingfu He and Weicheng Zhao and Zhongran Chi and Shusheng Zang},
keywords = {Conjugate heat transfer, Genetic algorithm, Film cooling, Generative adversarial network, Optimization},
abstract = {Cooling design optimization with complex nonuniform heat loads is a typical challenge in the development of new generation gas turbines. Combined inlet hot streaks and swirls caused by lean-burn combustor force film cooling to attenuate uneven heat loads on the blade surface, especially in the spanwise direction. Besides, the complex coupling relationship among hundreds of design variables of film cooling arrangement hinders the development of the optimal design. The deep learning model shows a strong fitting ability when dealing with high-dimensional nonlinear problems, which could fit the mapping relationship between design variables and the temperature field. In this paper, a turbine cooling design optimization methodology based on conjugate heat transfer (CHT) simulation and conditional generative adversarial network (cGAN) is developed, and the film cooling design of the 1st stage turbine vanes is optimized through the multi-objective genetic algorithm (MOGA). An initial sample containing 96 cases is constructed by CHT computational fluid dynamics (CFD) simulation with inlet hot streaks and swirls. Based on the initial sample, a cGAN model that predicts the temperature distribution of the vane surface is trained and tested. The film hole arrangement of the vane surface is described by a 276-bit binary optimization variable. The 5% maximum temperature predicted by cGAN and the regressed coolant mass flow is used as the dual optimization objectives. The optimal film hole arrangements in rows and the scattered film hole arrangements found by MOGA are compared, which shows that the optimal scattered arrangements perform better due to well adaptability to the nonuniform thermal load in the spanwise direction. The impact of sample size and sample selection on the performance of the cGAN model is discussed. The retrained cGAN model indicates that a proper abundance of specific samples can improve the prediction of complex coupling phenomena such as backflow.}
}
@article{DUONG201880,
title = {Application of multi-output Gaussian process regression for remaining useful life prediction of light emitting diodes},
journal = {Microelectronics Reliability},
volume = {88-90},
pages = {80-84},
year = {2018},
note = {29th European Symposium on Reliability of Electron Devices, Failure Physics and Analysis ( ESREF 2018 )},
issn = {0026-2714},
doi = {https://doi.org/10.1016/j.microrel.2018.07.106},
url = {https://www.sciencedirect.com/science/article/pii/S0026271418306723},
author = {Pham Luu Trung Duong and Hyunseok Park and Nagarajan Raghavan},
keywords = {Light emitting diode, Gaussian process regression, Prognostic health management, Remaining useful life},
abstract = {Light-emitting diodes (LEDs) are the preferred technology today when it comes to lighting both for indoor and outdoor applications, predominantly due to their high efficiency, environmental resilience and prolonged lifetime. Given their widespread use, there is a need to quickly qualify them and accurately predict the reliability of these devices. Due to their inherently long operational life, most LED reliability studies involve the use of degradation tests and application of filter-based prognostic techniques for dynamic update of degradation model parameters and estimation of the remaining useful life (RUL). Although they are in general very effective, the main drawback is the need for a specific state-space model that describes the degradation. In many cases, LED degradation trends are affected by a multitude of unknown factors such as unidentified failure modes, varying operational conditions, process and measurement variance, and environmental fluctuations. These variable factors that are hard to control tend to complicate the selection of a suitable state-space model and in some cases; there may not be a single model that could be used for the entire lifespan of the device. If the degradation patterns of LEDs under test deviate from the state space models, the resulting predictions will be inaccurate. This paper introduces a prognostics-based qualification method using a multi-output Gaussian process regression (MO-GPR) and applies it to RUL prediction of high-power LED devices. The main idea here is to use MO-GPR to learn the correlation between similar degradation patterns from multiple similar components under test and thereby, bypass the need for a specific state space model using available data of past units tested to failure.}
}
@article{THOMSON2023108275,
title = {Comparing the predictive ability of Sentinel-2 multispectral imagery and a proximal hyperspectral sensor for the estimation of pasture nutritive characteristics in an intensive rotational grazing system},
journal = {Computers and Electronics in Agriculture},
volume = {214},
pages = {108275},
year = {2023},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2023.108275},
url = {https://www.sciencedirect.com/science/article/pii/S0168169923006634},
author = {A. Thomson and J. Jacobs and E. Morse-McNabb},
keywords = {Sentinel-2, Satellite, Multispectral, Perennial Ryegrass, Nutritive Characteristics},
abstract = {Using remote sensing to estimate the nutritive characteristics of pasture in livestock systems is a rapidly developing area that has the potential to improve pasture management and utilisation and in turn improve livestock productivity. The Copernicus Sentinel-2 paired satellites have shown promise in predicting pasture nutritive characteristics and have a pixel size small enough to be able to identify intra-paddock nutrient variation in rotational dairy grazing systems where paddock sizes are small. This study sought to develop nutrient prediction models using a dataset of nine Sentinel-2A and -2B satellite images, by linking imagery to perennial ryegrass dairy pasture samples cut within known pixels of the images in the days directly after each cloud-free overpass. Just before destructive pasture sampling, a handheld, proximal, hyperspectral sensor was used to gather spectral signatures (400–2500 nm) from areas to be sampled. Pasture samples (n = 200) were analysed using wet chemistry techniques for seven nutritive characteristics. The dry matter concentration of the samples was also measured, creating eight variates to be modelled in total. The Sentinel-2 data from within the selected pixels was extracted and used to create input features for Random Forest prediction models. Input features included the raw multispectral bands, common vegetation indices, and independent variables ‘season’ and ‘month’. Predictive models were also developed from the hyperspectral sensor data using Partial Least Squares Regression. For both modelling types, variable selection was used to reduce model inputs to only those sensitive to each nutrient. Results comparing the final models in an independent validation test showed similar predictive performance of the Sentinel-2 satellite and the proximal hyperspectral sensor for all variates of interest, shown by mean absolute errors that were not significantly different. In each case, the dry matter concentration was the best predicted variable (Lin’s concordance correlation coefficient (LCCC) > 0.90) with excellent potential to be quantified by Sentinel-2 satellites. Crude protein, metabolisable energy, neutral detergent fibre, non-fibre carbohydrate, and water-soluble carbohydrate concentrations were also predicted with good to moderate accuracy by both sensors (LCCC between 0.50 and 0.80). These models would be sufficiently accurate for making qualitative predictions (e.g., grouping into high to low categories). Ash and acid detergent fibre concentrations were poorly predicted (LCCC < 0.50) and these models would not be recommended for further use. It was concluded that multispectral data from Sentinel-2 has great potential to estimate pasture nutrient concentrations for intensively grazed pastures such as those used in dairy systems in temperate regions.}
}
@article{SUN201441,
title = {Predicting financial distress and corporate failure: A review from the state-of-the-art definitions, modeling, sampling, and featuring approaches},
journal = {Knowledge-Based Systems},
volume = {57},
pages = {41-56},
year = {2014},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2013.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950705113003869},
author = {Jie Sun and Hui Li and Qing-Hua Huang and Kai-Yu He},
keywords = {Definition of financial distress, Sampling methods, Featuring methods, Review, Financial distress prediction, Corporate failure prediction, Case-based reasoning, Ensemble, Group decision-making, Support vector machine, Hybrid modeling, Neural network, Decision tree, Logistic regression, Multiple discriminant analysis},
abstract = {As a hot topic, financial distress prediction (FDP), or called as corporate failure prediction, bankruptcy prediction, acts as an important role in decision-making of various areas, including: accounting, finance, business, and engineering. Since academic research on FDP has gone on for nearly eighty years, there are abundant literatures on this topic, which may appear chaotic to the researchers of the field and make them feel confused. This paper contributes to the current review researches by making a full summary, analysis and evaluation on the current literatures of FDP. The current literatures of FDP are reviewed from the following four unique aspects: definition of financial distress in the new century, FDP modeling, sampling approaches for FDP, and featuring approaches for FDP. By considering the new state-of-the-art techniques in this area, FDP modeling are classified and reviewed by the following groups: namely, modeling with pure single classifier, modeling with hybrid single classifier, modeling by ensemble techniques, dynamic FDP modeling, and modeling with group decision-making techniques. Sampling methods for FDP are classified and reviewed by the following paired groups, namely: training sampling and testing sampling, single industry sampling and cross-industry sampling, balanced sampling and imbalanced sampling. Featuring methods for FDP are categorized and reviewed by qualitative selection and combination of qualitative and quantitative selection. We comment on the current researches from the view of each category and propose further research topics. The review paper is valuable to guide research and application of the area.}
}
@article{SHAHINFAR2019159,
title = {Prediction of sheep carcass traits from early-life records using machine learning},
journal = {Computers and Electronics in Agriculture},
volume = {156},
pages = {159-177},
year = {2019},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2018.11.021},
url = {https://www.sciencedirect.com/science/article/pii/S0168169918309736},
author = {Saleh Shahinfar and Khama Kelman and Lewis Kahn},
keywords = {Machine learning, Deep learning, Carcass weight, Intramuscular fat, Sheep},
abstract = {Currently hot carcass weight (HCW) and fat score jointly indicate the price grid for sheep meat in Australia. However, experts in the field believe that soon, yield and quality traits such as intramuscular fat (IMF), greville rule fat depth (GRFAT), computed tomography lean meat yield (CTLEAN), and loin weight (LW) are likely to play a role in pricing. Having an accurate prediction of these traits earlier in the life of an animal will allow sheep producers to adjust their management practices in order to achieve the target market requirements. Management, genetics, pasture and climate factors, influence these traits directly and epistatically. Traditional prediction methods may not be powerful enough to capture complex interactions while avoiding overfitting. In this case, learning algorithms that can learn from the current data to predict the animal’s future performance offers promise. In this study, five different types of Machine Learning (ML) algorithm, namely Deep Learning (DL), Gradient Boosting Tree (GBT), K-Nearest Neighbour (KNN), Model Tree (MT), and Random Forest (RF) were employed to predict HCW, IMF, GRFAT, LW and CTLEAN and their performances were compared against linear regression (LR) as the gold standard of multinomial prediction. Four scenarios representing different numbers of weight recordings -from a total of 9 weight measures taken between birth (WT1) and pre-slaughter (WT9)- were used to inform the algorithms and all models were trained and tested under equal conditions with identical training and testing sets. Selection of the most effective subset of predictor features were completed via greedy stepwise search among all the available features jointly with expert opinion. In predicting all the traits, RF was superior while LR and KNN showed the lowest prediction performance. When using the final model for predicting on an independent test set, the scenario with the most accurate prediction performance differed across traits. IMF and GRFAT were most accurately predicted when using birth, weaning, and pre-slaughter weights, while the most accurate scenario for HCW, LW and CTLEAN utilised weaning, six monthly weight measures after weaning and pre-slaughter weight. Across all scenarios the least accurate prediction was for IMF.}
}
@article{SHAH2018369,
title = {A spectroscopic chemometric modeling approach based on statistics pattern analysis},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {18},
pages = {369-374},
year = {2018},
note = {10th IFAC Symposium on Advanced Control of Chemical Processes ADCHEM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.09.328},
url = {https://www.sciencedirect.com/science/article/pii/S240589631832010X},
author = {Devarshi Shah and Q. {Peter He} and Jin Wang},
keywords = {Soft sensor, Variable selection, Multivariate regression, Partial least squares, Statistics pattern analysis, NIR, UV/Vis, Chemometrics},
abstract = {Spectroscopic techniques such as near-infrared spectroscopy have gained wide applications in the last few decades. As a result, various soft sensors have been developed to predict sample properties from the sample’s spectroscopic readings. Because the readings at different wavelengths are highly correlated, it has been shown that variable selection could significantly improve a soft sensor’s prediction performance and reduce the model complexity. Currently, almost all variable selection methods focus on how to select the variables (i.e., wavelengths or wavelength segments) that are strongly correlated with the dependent variable to improve the prediction performance. Although many successful applications have been reported, such variable selection methods do have their limitations, such as high sensitivity to the choice of training data, and poorer performance when testing on new samples. This is because the variables that are removed from model building may contain useful information about the sample property. To address this limitation, we propose a statistics pattern analysis (SPA) based method. Instead of selecting certain wavelengths or wavelength segments, the SPA-based method considers the whole spectrum which is divided into segments, and extracts different features over each spectrum segment to build the soft sensor. Two case studies are presented to demonstrate the performance of the SPA-based soft sensor and compared with a full partial least squares (PLS) model, and a synergy interval PLS (SiPLS) model.}
}
@article{BALDACCHINO2016178,
title = {Variational Bayesian mixture of experts models and sensitivity analysis for nonlinear dynamical systems},
journal = {Mechanical Systems and Signal Processing},
volume = {66-67},
pages = {178-200},
year = {2016},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2015.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0888327015002307},
author = {Tara Baldacchino and Elizabeth J. Cross and Keith Worden and Jennifer Rowson},
keywords = {Mixture of experts, Variational Bayesian training, Sensitivity analysis, Nonlinear bifurcating systems, Model selection},
abstract = {Most physical systems in reality exhibit a nonlinear relationship between input and output variables. This nonlinearity can manifest itself in terms of piecewise continuous functions or bifurcations, between some or all of the variables. The aims of this paper are two-fold. Firstly, a mixture of experts (MoE) model was trained on different physical systems exhibiting these types of nonlinearities. MoE models separate the input space into homogeneous regions and a different expert is responsible for the different regions. In this paper, the experts were low order polynomial regression models, thus avoiding the need for high-order polynomials. The model was trained within a Bayesian framework using variational Bayes, whereby a novel approach within the MoE literature was used in order to determine the number of experts in the model. Secondly, Bayesian sensitivity analysis (SA) of the systems under investigation was performed using the identified probabilistic MoE model in order to assess how uncertainty in the output can be attributed to uncertainty in the different inputs. The proposed methodology was first tested on a bifurcating Duffing oscillator, and it was then applied to real data sets obtained from the Tamar and Z24 bridges. In all cases, the MoE model was successful in identifying bifurcations and different physical regimes in the data by accurately dividing the input space; including identifying boundaries that were not parallel to coordinate axes.}
}
@article{PAULUS201595,
title = {Algorithm for automating the selection of a temperature dependent change point model},
journal = {Energy and Buildings},
volume = {87},
pages = {95-104},
year = {2015},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2014.11.033},
url = {https://www.sciencedirect.com/science/article/pii/S0378778814009645},
author = {Mitchell T. Paulus and David E. Claridge and Charles Culp},
keywords = {Energy modeling, Regression, EnergyPlus, Measurement and verification},
abstract = {An algorithm was developed to automate the process of selecting a temperature dependent change point model. Regression models based solely on outdoor air temperature for monitoring and verification purposes are common. The correct change point model shape is determined through a series of three tests. The first test checks whether the coefficients of the model are the correct sign for the shape. The second test checks if the coefficients for the model are significant. The final test checks whether enough data points are present in each temperature region of the model. The algorithm was tested with synthetic EnergyPlus electricity and natural gas data for an outpatient hospital, medium office building, large office building, large hotel, secondary school, and warehouse, with weather data from Chicago, Miami, Seattle, and Fairbanks. The algorithm was able to select the most appropriate temperature dependent change point model for all 48 cases tested. The algorithm can be used in an automated energy modeling routine for monitoring and verification or for checking human decision-making in the energy modeling process.}
}
@article{BUASON2022108579,
title = {A sample-based approach for computing conservative linear power flow approximations},
journal = {Electric Power Systems Research},
volume = {212},
pages = {108579},
year = {2022},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2022.108579},
url = {https://www.sciencedirect.com/science/article/pii/S0378779622006526},
author = {Paprapee Buason and Sidhant Misra and Daniel K. Molzahn},
keywords = {Conservative linear approximation, Sample selection, Power flow approximation},
abstract = {Non-convexities induced by the non-linear power flow equations challenge solution algorithms for many power system optimization and control problems. Linear approximations are often used to address these challenges by trading off modeling accuracy for tractability. The accuracy of a power flow linearization depends on the characteristics of the power system and the operational range where the linearization is applied. However, rather than exploiting knowledge of these characteristics for a particular system, many existing power flow linearizations are based on general assumptions for broad classes of systems, thus limiting their accuracy. Moreover, since existing linearizations do not consistently overestimate or underestimate quantities of interest such as voltage magnitudes and line flows, algorithms based on these linearizations may lead to constraint violations when applied to the system. In contrast, this paper computes conservative linear approximations of the power flow equations, i.e., linear approximations that intend to overestimate or underestimate a quantity of interest in order to enable tractable algorithms that avoid constraint violations. Using a sample-based approach, we compute these conservative linearizations by solving a constrained linear regression problem. We analyze and improve the conservative linear approximations via an iterative sampling approach, optimizing over functions of the quantities of interest, and a sample-complexity analysis. Considering the relationships between the voltage magnitudes and the active and reactive power injections, we characterize the performance of the conservative linear approximations for a range of test cases.}
}
@article{PEJOVIC20181,
title = {Sparse regression interaction models for spatial prediction of soil properties in 3D},
journal = {Computers & Geosciences},
volume = {118},
pages = {1-13},
year = {2018},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2018.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S009830041730852X},
author = {Milutin Pejović and Mladen Nikolić and Gerard B.M. Heuvelink and Tomislav Hengl and Milan Kilibarda and Branislav Bajat},
keywords = {Spatial prediction, Lasso, Interactions, Nested cross-validation, Soil organic carbon, 3D},
abstract = {An approach for using lasso (Least Absolute Shrinkage and Selection Operator) regression in creating sparse 3D models of soil properties for spatial prediction at multiple depths is presented. Modeling soil properties in 3D benefits from interactions of spatial predictors with soil depth and its polynomial expansion, which yields a large number of model variables (and corresponding model parameters). Lasso is able to perform variable selection, hence reducing the number of model parameters and making the model more easily interpretable. This also prevents overfitting, which makes the model more accurate. The presented approach was tested using four variable selection approaches – none, stepwise, lasso and hierarchical lasso, on four kinds of models – standard linear model, linear model with polynomial expansion of depth, linear model with interactions of covariates with depth and linear model with interactions of covariates with depth and its polynomial expansion. This framework was used to predict Soil Organic Carbon (SOC) in three contrasting study areas: Bor (Serbia), Edgeroi (Australia) and the Netherlands. Results show that lasso yields substantial improvements in accuracy over standard and stepwise regression — up to 50 % of total variance. It yields models which contain up to five times less nonzero parameters than the full models and that are usually more sparse than models obtained by stepwise regression, up to three times. Extension of the standard linear model by including interactions typically improves the accuracy of models produced by lasso, but is detrimental to standard and stepwise regression. Regarding computation time, it was demonstrated that lasso is several orders of magnitude more efficient than stepwise regression for models with tens or hundreds of variables (including interactions). Proper model evaluation is emphasized. Considering the fact that lasso requires meta-parameter tuning, standard cross-validation does not suffice for adequate model evaluation, hence a nested cross-validation was employed. The presented approach is implemented as publicly available sparsereg3D R package.}
}
@article{KATHARI2020275,
title = {Scalar correlation functions for model structure selection in high-dimensional time-series modelling},
journal = {ISA Transactions},
volume = {100},
pages = {275-288},
year = {2020},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2019.11.033},
url = {https://www.sciencedirect.com/science/article/pii/S0019057819305312},
author = {Sudhakar Kathari and Arun K. Tangirala},
keywords = {Model selection, Order determination, Scalar correlation functions, High-dimensional, Time-series modelling, VARMA models},
abstract = {Model structure selection is an important step in high-dimensional time-series modelling. Traditionally AIC and BIC have been used for this purpose, however, only post model estimation. On the other hand, modern approaches use penalized regression methods, but the optimization is in a user-specified model class. In this work, we propose a pre-estimation approach based on two novel correlation functions, namely, the scalar autocorrelation function (SACF) and the scalar inverse autocorrelation function (SIACF) for identifying the appropriate model class among the vector autoregressive (VAR), vector moving average (VMA), and mixed (VARMA) classes. In addition, these scalar functions theoretically provide the exact order of VAR and VMA processes, and are computationally feather light even for high-dimensional series. The proposed functions are obtained through two linear constructs of the given multivariate process with a lagged-correlation equivalence constraint. The key benefit is that only two correlation functions need to be examined as against the standard M2 correlation and M2 inverse (or partial) correlation plots for an M-dimensional process. This benefit extends to conducting whiteness test in multivariate time-series modelling and is particularly pronounced under small sample conditions, wherein parsimony, structure and class of the identified model is crucial in achieving efficient estimates. Theoretical proofs and case studies are presented to establish the properties and to demonstrate the utility of proposed correlation functions.}
}
@article{ELMASRY202167,
title = {Selection of representative hyperspectral data and image pretreatment for model development in heterogeneous samples: A case study in sliced dry-cured ham},
journal = {Biosystems Engineering},
volume = {201},
pages = {67-82},
year = {2021},
issn = {1537-5110},
doi = {https://doi.org/10.1016/j.biosystemseng.2020.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S1537511020303081},
author = {Gamal M. ElMasry and Elena Fulladosa and Josep Comaposada and Salim S. Al-Rejaie and Pere Gou},
keywords = {Chemical imaging, Dry-cured ham, Hyperspectral imaging, Multivariate analysis, PLS, ROI},
abstract = {Sliced dry-cured ham arranged in ready-to-eat packages is a convenient and widely consumed commodity characterised by heterogeneity in composition not only among different industrial batches but also through their horizontal and vertical profiles, making precise nutrition labelling of the packages a difficult task. Hyperspectral imaging techniques can serve as a steadfast solution not only to predict the overall composition of the major constituents of dry-cured ham but also to visualise their distributions. The main aim of this study was to define the optimal protocol for pretreating hyperspectral images and selecting representative hyperspectral data for developing accurate predictive models in excessively heterogeneous samples, using sliced dry-cured ham as a case study. Hyperspectral images (400–1000 nm) were acquired for heterogeneous sliced dry-cured ham and homogeneous unsliced dry-cured muscles. Partial least squares (PLS) regression models to predict fat, water, salt and protein contents were developed and tested in an independent dataset. The PLS predictive models developed from the whole surface of sliced dry-cured ham were the most accurate ones for predicting fat, water, salt and protein contents with a determination coefficient in prediction (Rp2) of 0.89, 0.85, 83 and 0.63 and standard error in prediction (SEP) of 1.43, 1.21, 0.51 and 1.57%, respectively. The chemical images resulting from the models gave advantages of hyperspectral imaging technique over traditional chemical methods to visualise the spatial distribution of different constituents within the packaged ham slices.}
}
@article{DIMARTINO20168,
title = {A simplified computational fluid-dynamic approach to the oxidizer injector design in hybrid rockets},
journal = {Acta Astronautica},
volume = {129},
pages = {8-21},
year = {2016},
issn = {0094-5765},
doi = {https://doi.org/10.1016/j.actaastro.2016.08.026},
url = {https://www.sciencedirect.com/science/article/pii/S0094576516306762},
author = {Giuseppe D. {Di Martino} and Paolo Malgieri and Carmine Carmicino and Raffaele Savino},
keywords = {Hybrid rocket motor, Fuel regression rate, Oxidizer injection, Numerical simulation},
abstract = {Fuel regression rate in hybrid rockets is non-negligibly affected by the oxidizer injection pattern. In this paper a simplified computational approach developed in an attempt to optimize the oxidizer injector design is discussed. Numerical simulations of the thermo-fluid-dynamic field in a hybrid rocket are carried out, with a commercial solver, to investigate into several injection configurations with the aim of increasing the fuel regression rate and minimizing the consumption unevenness, but still favoring the establishment of flow recirculation at the motor head end, which is generated with an axial nozzle injector and has been demonstrated to promote combustion stability, and both larger efficiency and regression rate. All the computations have been performed on the configuration of a lab-scale hybrid rocket motor available at the propulsion laboratory of the University of Naples with typical operating conditions. After a preliminary comparison between the two baseline limiting cases of an axial subsonic nozzle injector and a uniform injection through the prechamber, a parametric analysis has been carried out by varying the oxidizer jet flow divergence angle, as well as the grain port diameter and the oxidizer mass flux to study the effect of the flow divergence on heat transfer distribution over the fuel surface. Some experimental firing test data are presented, and, under the hypothesis that fuel regression rate and surface heat flux are proportional, the measured fuel consumption axial profiles are compared with the predicted surface heat flux showing fairly good agreement, which allowed validating the employed design approach. Finally an optimized injector design is proposed.}
}
@article{LIU2016796,
title = {An adaptive online learning approach for Support Vector Regression: Online-SVR-FID},
journal = {Mechanical Systems and Signal Processing},
volume = {76-77},
pages = {796-809},
year = {2016},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2016.02.056},
url = {https://www.sciencedirect.com/science/article/pii/S0888327016001072},
author = {Jie Liu and Enrico Zio},
keywords = {Online learning, Support Vector Regression, Time series data, Pattern drift, Feature vector selection, Incremental and decremental learning},
abstract = {Support Vector Regression (SVR) is a popular supervised data-driven approach for building empirical models from available data. Like all data-driven methods, under non-stationary environmental and operational conditions it needs to be provided with adaptive learning capabilities, which might become computationally burdensome with large datasets cumulating dynamically. In this paper, a cost-efficient online adaptive learning approach is proposed for SVR by combining Feature Vector Selection (FVS) and Incremental and Decremental Learning. The proposed approach adaptively modifies the model only when different pattern drifts are detected according to proposed criteria. Two tolerance parameters are introduced in the approach to control the computational complexity, reduce the influence of the intrinsic noise in the data and avoid the overfitting problem of SVR. Comparisons of the prediction results is made with other online learning approaches e.g. NORMA, SOGA, KRLS, Incremental Learning, on several artificial datasets and a real case study concerning time series prediction based on data recorded on a component of a nuclear power generation system. The performance indicators MSE and MARE computed on the test dataset demonstrate the efficiency of the proposed online learning method.}
}
@incollection{BENER2015453,
title = {Chapter 16 - Lessons Learned from Software Analytics in Practice},
editor = {Christian Bird and Tim Menzies and Thomas Zimmermann},
booktitle = {The Art and Science of Analyzing Software Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {453-489},
year = {2015},
isbn = {978-0-12-411519-4},
doi = {https://doi.org/10.1016/B978-0-12-411519-4.00016-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780124115194000161},
author = {Ayse Bener and Ayse Tosun Misirli and Bora Caglayan and Ekrem Kocaguneli and Gul Calikli},
keywords = {Software analytics framework, Industry research projects, Data extraction, Descriptive statistics, Predictive analytics, Prescriptive analytics},
abstract = {In this chapter, we share our experience and views on software data analytics in practice with a review of our previous work. In more than 10 years of joint research projects with industry, we have encountered similar data analytics patterns in diverse organizations and in different problem cases. We discuss these patterns following a “software analytics” framework: problem identification, data collection, descriptive statistics, and decision making. In the discussion, our arguments and concepts are built around our experiences of the research process in six different industry research projects in four different organizations. Methods: Spearman rank correlation, Pearson correlation, Kolmogorov-Smirnov test, chi-square goodness-of-fit test, t test, Mann-Whitney U test, Kruskal-Wallis analysis of variance, k-nearest neighbor, linear regression, logistic regression, naïve Bayes, neural networks, decision trees, ensembles, nearest-neighbor sampling, feature selection, normalization.}
}
@article{DING2020497,
title = {An integrated method based on relevance vector machine for short-term load forecasting},
journal = {European Journal of Operational Research},
volume = {287},
number = {2},
pages = {497-510},
year = {2020},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2020.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S0377221720303404},
author = {Jia Ding and Maolin Wang and Zuowei Ping and Dongfei Fu and Vassilios S. Vassiliadis},
keywords = {Short-term load forecasting, Relevance vector machine, Machine learning, Wavelet transform, Feature selection},
abstract = {Short-term electricity load forecasting has become increasingly important due to the privatization and deregulation in the energy market. This study proposes a probabilistic learning method to predict hour-ahead and day-ahead load demand. Unlike methods in previous studies, the proposed method integrates wavelet transform and feature selection as key preprocessing steps. Features are divided into current state related features and historical information related features. Current state related features are forecasted by the regression model before being added into the load prediction model. The entire learning and prediction process is based on the relevance vector machine (RVM) that utilizes load data characteristics. A number of test cases are presented using benchmark datasets from the New York Independent System Operator (NYISO) and ISO New England. Based on the detailed empirical comparison, the proposed RVM-based integrated method outperforms classical time series approaches and state-of-the-art artificial intelligence methods on short-term load forecasting.}
}
@article{KORKUT2018543,
title = {Selection of catalyst and reaction conditions for ultrasound assisted biodiesel production from canola oil},
journal = {Renewable Energy},
volume = {116},
pages = {543-551},
year = {2018},
issn = {0960-1481},
doi = {https://doi.org/10.1016/j.renene.2017.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S0960148117309710},
author = {Ibrahim Korkut and Mahmut Bayramoglu},
keywords = {Biodiesel synthesis, Heterogeneous catalyst, Ultrasound, D-optimal plan, Optimization},
abstract = {In this study, the experimental optimization of ultrasound assisted biodiesel production in presence of heterogeneous catalyst was investigated. Three catalysts namely; CaO, calcined dolomite and calcium diglyceroxide (CaDG) were taken into account for comparative purpose. D-optimal experimental plan was applied to obtain regression models which were subsequently used for the detection of optimum process conditions. Maximum biodiesel yields were calculated as 98.7%, 95.9% and 86.3% for CaO, calcined dolomite and CaDG respectively. Furthermore, supplemental experiments were conducted around optimum process conditions to test the validity of the regression models and to refine the optimum results. Finally, in the case of CaO catalyst, maximum biodiesel yield (99.4%) was obtained at the following conditions; catalyst loading: 5.35% (wt.of oil), methanol/oil ratio: 7.48, ultrasonic power: 40 W, time: 150 min, and reaction temperature: 60 °C.}
}
@article{ARANDIGA20132474,
title = {Learning-based multiresolution transforms with application to image compression},
journal = {Signal Processing},
volume = {93},
number = {9},
pages = {2474-2484},
year = {2013},
issn = {0165-1684},
doi = {https://doi.org/10.1016/j.sigpro.2013.03.020},
url = {https://www.sciencedirect.com/science/article/pii/S0165168413000959},
author = {Francesc Aràndiga and Albert Cohen and Dionisio F. Yáñez},
keywords = {Multiresolution transforms, Statistical learning theory, Linear regression, Signal decompositions, Image decompositions},
abstract = {In Harten's framework, multiresolution transforms are defined by predicting finer resolution levels of information from coarser ones using an operator, called prediction operator, and defining details (or wavelet coefficients) that are the difference between the exact and predicted values. In this paper we use tools of statistical learning in order to design a more accurate prediction operator in this framework based on a training sample, resulting in multiresolution decompositions with enhanced sparsity. In the case of images, we incorporate edge detection techniques in the design of the prediction operator in order to avoid Gibbs phenomenon. Numerical tests are presented showing that the learning-based multiresolution transform compares favorably with the standard multiresolution transforms in terms of compression capability.}
}
@article{RIBEIRO2016140,
title = {Assessment of epistemic uncertainties in the shear strength of slender reinforced concrete beams},
journal = {Engineering Structures},
volume = {116},
pages = {140-147},
year = {2016},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2016.02.045},
url = {https://www.sciencedirect.com/science/article/pii/S0141029616300037},
author = {A.B. Ribeiro and J.M.F. Calixto and S.M.C. Diniz},
keywords = {Beams, Epistemic uncertainties, Regression analysis, Reinforced concrete, Shear strength, Shear database},
abstract = {Selection of an appropriate model for shear capacity of reinforced concrete (RC) slender beams poses a number of challenges. First, different models with different levels of conservatism have been proposed in an attempt to describe shear resistance. Second, according to Reineck et al. (2014), code provisions for shear capacity of RC beams with shear reinforcement have been primarily derived from test data with respect to the required amount of shear reinforcement and the calculation of maximum shear capacity. Third, current models have been developed based on databases presenting two major drawbacks: (i) most data points are crowded in the small size range, and (ii) the means of the subsidiary influencing parameters are very different within different intervals of beam size (or beam depth). In this study, a filtered database is used in such a way to circumvent the drawbacks mentioned above. A random variable “model error”, i.e. ratio experimental to predicted shear strength, is associated to each of the shear models analyzed in this work (NBR 6118, ACI 318, EUROCODE 2, and CSA A.23.3). It was observed that in some cases, most notably for the effective depth, a trend exists for a decrease in the “model error” as the effective depth increases. Considering the limitations of the four analyzed models, a nonlinear regression model was proposed. The database presented by Reineck et al. (2014) was used in the assessment of the effectiveness and accuracy of the proposed regression model. No trend was found associated to the most significant variables in the shear strength prediction, i.e. a uniform level of conservatism is attained throughout the range of these variables. The regression model proposed herein and the attendant statistics of the model error (mean, coefficient of variation and type of distribution) can be easily used in a reliability analysis procedure to assess safety levels implicit in different design procedures.}
}
@article{ZHOU2022103840,
title = {Quantitative analysis of contrast-enhanced ultrasound combined with ultrasound in the unifocal papillary thyroid micro-carcinoma},
journal = {Medical Engineering & Physics},
volume = {110},
pages = {103840},
year = {2022},
issn = {1350-4533},
doi = {https://doi.org/10.1016/j.medengphy.2022.103840},
url = {https://www.sciencedirect.com/science/article/pii/S1350453322000893},
author = {Xiaohui Zhou and Min Zhang and Linyuan Jin and Xianpeng Tang and Qiang Hu and Guanghui Cheng and Yaocheng Xiao},
keywords = {Unifocal papillary thyroid microcarcinoma, Invasiveness, Us, ceus, diagnostic performance},
abstract = {Objective
To evaluate diagnostic value of ultrasound (US) combined with contrast-enhanced ultrasound (CEUS) in the invasiveness of unifocal papillary thyroid micro-carcinoma (UPTMC) without capsule-invasion.
Methods
This retrospective study included data from patients with UPTMC who received US and CEUS examinations in the Ultrasound Department of the Central Hospital of Changsha, China between June 2019 and September 2021. Univariate and multivariate logistic regression analysis were used to evaluate the risk of US and CEUS parameters for UPTMC. Diagnostic performance was estimated by ROC analysis.
Results
A total of 136 cases were enrolled, including invasive UPTMC (n = 47) and non-invasive UPTMC (n = 89), which were divided into test set (n = 109) and validation set (n = 27). The occurrence of microcalcification and the ratios (R) of each time-intensity curve (TIC) of CEUS parameter were significantly higher in patients with invasive UTPMC than non-invasive UPTMC (all P < 0.05). Additionally, nodular diameter was significantly longer in the invasive group (P < 0.05). Multivariate analysis showed that microcalcification (OR = 2.917, 95% CI: 1.002–8.491, P = 0.050), R-TTP > 1 (OR = 3.376, 95%CI: 1.267–8.994, P = 0.015), R-DS > 1 (OR = 6.558, 95% CI: 2.358–18.243, P < 0.010) were independently associated with invasive UPTMC. The sensitivities of US, CEUS and their combined application were 82.1%, 46.2% and 79.5%, respectively, and their specificities were 37.1%, 88.6% and 61.4%, respectively. The combination of the two methods had the best diagnostic efficiency (AUC=0.775)compared to US (AUC = 0.596) and CEUS (AUC = 0.750).
Conclusion
The combination of US and CEUS might have good diagnostic value for UPTMC with capsule non-invasion.}
}
@article{TAYLOR2021104434,
title = {Finite element analysis informed variable selection for femoral fracture risk prediction},
journal = {Journal of the Mechanical Behavior of Biomedical Materials},
volume = {118},
pages = {104434},
year = {2021},
issn = {1751-6161},
doi = {https://doi.org/10.1016/j.jmbbm.2021.104434},
url = {https://www.sciencedirect.com/science/article/pii/S1751616121001211},
author = {Mark Taylor and Marco Viceconti and Pinaki Bhattacharya and Xinshan Li},
abstract = {Logistic regression classification (LRC) is widely used to develop models to predict the risk of femoral fracture. LRC models based on areal bone mineral density (aBMD) alone are poor, with area under the receiver operator curve (AUROC) scores reported to be as low as 0.63. This has led to researchers investigating methods to extract further information from the image to increase performance. Recently, the use of active shape (ASM) and appearance models (AAM) have resulted in moderate improvements, but there is a risk that inclusion of too many modes will lead to overfitting. In addition, there are concerns that the effort required to extract the additional information does not justify the modest improvement in fracture risk prediction. This raises the question, are we reaching the limits of the information that can be extracted from an image? Finite element analysis was used in combination with active shape and appearance modelling to select variables to develop LRC models of fracture risk. Active shape and active appearance models were constructed based on a previously reported cohort of 94 post-menopausal Caucasian women (47 with and 47 without a fracture). T-tests were used to identify differences between the two groups for each mode of variation. Femur strength was predicted for two load cases, stance and a fall. Stepwise multi-variate linear regression was used to identify shape and appearance modes that were predictors of strength for the femurs in the training set. Femurs were also synthetically generated to explore the influence of the first 10 modes of the shape and appearance models. Identified modes of variation were then used to generate LRC models to predict fracture risk. Only 6 modes, 4 active appearance and 2 active shape modes, were identified that had a significant influence on predicted fracture strength. Of these, only two active appearance modes were needed to substantially improve the predictive mode performance (ΔAUROC = 0.080). The addition of 3 more modes (1 AAM and two ASM) further improved the performance of the classifier (ΔAUROC = 0.123). Further addition of modes did not result in any further substantial improvements. Based on these findings, it is suggested that we are reaching the limits of the information that can be extracted from an image to predict fracture risk.}
}
@article{ROY201916,
title = {A methodology for customizing clinical tests for esophageal cancer based on patient preferences},
journal = {Artificial Intelligence in Medicine},
volume = {95},
pages = {16-26},
year = {2019},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2018.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0933365716305449},
author = {Asis Roy and Sourangshu Bhattacharya and Kalyan Guin},
keywords = {Personalized diagnosis, Personalized test selection, Esophageal cancer, Classification with costs, Unbalanced classification, Electronic health record (EHR), Electronic medical record (EMR)},
abstract = {Background
Clinical tests for diagnosis of any disease may be expensive, uncomfortable, time consuming and can have side effects e.g. barium swallow test for esophageal cancer. Although we can predict non-existence of esophageal cancer with near 100% certainty just using demographics, lifestyle, medical history information, and a few basic clinical tests but our objective is to devise a general methodology for customizing tests with user preferences to avoid expensive or uncomfortable tests.
Method
We propose to use classifiers trained from electronic medical records (EMR) for selection of tests. The key idea is to design classifiers with 100% false normal rates, possibly at the cost of higher false abnormal. We find kernel logistic regression to be most suitable for the task. We propose an algorithm for finding the best probability threshold for kernel LR, based on test set accuracy tuning with help of a validation data set. Using the proposed algorithm, we describe schemes for selecting tests, which appear as features in the automatic classification algorithm, using preferences on costs and discomfort of the users i.e the proposed method is able to detect almost all true patients in the population even with user preferred clinical tests.
Result
We test our methodology with EMRs collected for more than 3000 patients, as a part of project carried out by a reputed hospital in Mumbai, India. We found that kernel SVM and kernel LR with a polynomial kernel of degree 3, yields an accuracy of 99.18% and sensitivity 100% using only demographic, lifestyle, patient history, and basic clinical tests. We demonstrate our test selection algorithm using two case studies, one using cost of clinical tests, and other using “discomfort” values for clinical tests. We compute the test sets corresponding to the lowest false abnormals for each criterion described above, using exhaustive enumeration of 12 and 15 clinical tests respectively. The sets turn out to be different, substantiating our claim that one can customize test sets based on user preferences.}
}
@incollection{BERNARDOGOIS20211,
title = {1 - Predictive models to the COVID-19},
editor = {Utku Kose and Deepak Gupta and Victor Hugo C. {de Albuquerque} and Ashish Khanna},
booktitle = {Data Science for COVID-19},
publisher = {Academic Press},
pages = {1-24},
year = {2021},
isbn = {978-0-12-824536-1},
doi = {https://doi.org/10.1016/B978-0-12-824536-1.00023-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012824536100023X},
author = {Francisco Nauber {Bernardo Gois} and Alex Lima and Khennedy Santos and Ramses Oliveira and Valdir Santiago and Saulo Melo and Rafael Costa and Marcelo Oliveira and Francisco das Chagas Douglas Marques Henrique and José Xavier Neto and Carlos Roberto {Martins Rodrigues Sobrinho} and João Alexandre {Lôbo Marques}},
keywords = {COVID-19, Forecast, Holt Winters, Kalman filter, Machine learning, Prophet, SEIR},
abstract = {Following the World Health Organization proclaims a pandemic due to a disease that originated in China and advances rapidly across the globe, studies to predict the behavior of epidemics have become increasingly popular, mainly related to COVID-19. The critical point of these studies is to discuss the disease's behavior and the progression of the virus's natural course. However, the prediction of the actual number of infected people has proved to be a difficult task, due to a wide range of factors, such as mass testing, social isolation, underreporting of cases, among others. Therefore, the objective of this work is to understand the behavior of COVID-19 in the state of Ceará to forecast the total number of infected people and to aid in government decisions to control the outbreak of the virus and minimize social impacts and economics caused by the pandemic. So, to understand the behavior of COVID-19, this work discusses some forecast techniques using machine learning, logistic regression, filters, and epidemiologic models. Also, this work brings a new approach to the problem, bringing together data from Ceará with those from China, generating a hybrid dataset, and providing promising results. Finally, this work still compares the different approaches and techniques presented, opening opportunities for future discussions on the topic. The study obtains predictions with R2 score of 0.99 to short-term predictions and 0.93 to long-term predictions.}
}
@article{DANG2021103345,
title = {Predicting tensile-shear strength of nugget using M5P model tree and random forest: An analysis},
journal = {Computers in Industry},
volume = {124},
pages = {103345},
year = {2021},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2020.103345},
url = {https://www.sciencedirect.com/science/article/pii/S0166361520305790},
author = {Subrat Kumar Dang and Kulwant Singh},
keywords = {Tensile-shear strength prediction, Machine-learning, Random forest (RFs), M5P model tree, Resistance spot welding (RSW)},
abstract = {Predicting the outcomes of a weld based on few metals with respect to its process parameters is a trivial phenomenon. However, the prediction requires complex mathematical formulation when the number of metals grows. The exponential rise in testing data of welding components in recent time, have increased the data inconsistency and complexity by manifolds. Further, the multi-physical characteristic of welding data adds to its chaotic nature. This makes manual or simulation-based extraction of useful information from welded data extremely challenging. Developing predictive models for tensile-shear strength of Resistance Spot Welding (RSW) is highly latency-bound. The recent success of machine learning approaches in variety of fields gives us motivation to address this issue. In this paper, we proposed a machine learning model inspired from random forest (RF) which predicts the tensile-shear strength of nugget from its input parameters and large number of metals. We trained the prediction model using data from 435 spot-welding cases and compared its performance with widely used M5P model tree. For all cases, RF-based prediction model outperforms the M5P model in terms of accuracy. Four different feature extraction techniques namely manual feature selection, correlation attribute eval., classification attribute eval., and reliefF attribute eval. were investigated to improve the performance of random forest model. From these methods, when model is very complex i.e. higher training size, classification attribute eval. provides greater accuracy with RMSETest of 0.5442. Moreover, no overfitting and underfitting was observed in this prediction.}
}
@article{TEISSEYRE2019290,
title = {Cost-sensitive classifier chains: Selecting low-cost features in multi-label classification},
journal = {Pattern Recognition},
volume = {86},
pages = {290-319},
year = {2019},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2018.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S0031320318303388},
author = {Paweł Teisseyre and Damien Zufferey and Marta Słomka},
keywords = {Multi-label classification, Cost-sensitive feature selection, Classifier chains, Logistic regression, Stability, Generalization error bounds},
abstract = {Feature selection is one of the trending challenges in multi-label classification. In recent years a lot of methods have been proposed. However the existing approaches assume that all the features have the same cost. This assumption may be inappropriate when the acquisition of the feature values is costly. For example in medical diagnosis each diagnostic value extracted by a clinical test is associated with its own cost. In such cases it may be better to choose a model with an acceptable classification performance but a much lower cost. We propose a novel method which incorporates the feature cost information into the learning process. The method, named Cost-Sensitive Classifier Chains, combines classifier chains and penalized logistic regression with a modified elastic-net penalty which takes into account costs of the features. We prove the stability and provide a bound on generalization error of our algorithm. We also propose the adaptive version in which penalty factors are changing during fitting the consecutive models in the chain. The methods are applied on real datasets: MIMIC-II and Hepatitis for which the cost information is provided by experts. Moreover, we propose an experimental framework in which the features are observed with measurement errors and the costs depend on the quality of the features. The framework allows to compare the cost-sensitive methods on benchmark datasets for which the cost information is not provided. The proposed method can be recommended in a situation when one wants to balance low costs and high prediction performance.}
}
@article{ENGSTROM2013581,
title = {Test overlay in an emerging software product line – An industrial case study},
journal = {Information and Software Technology},
volume = {55},
number = {3},
pages = {581-594},
year = {2013},
note = {Special Issue on Software Reuse and Product Lines},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2012.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S0950584912001061},
author = {Emelie Engström and Per Runeson},
keywords = {Product-line, Software testing, Case study, Overlay, Redundancy, Efficiency},
abstract = {Context
In large software organizations with a product line development approach, system test planning and scope selection is a complex task. Due to repeated testing: across different testing levels, over time (test for regression) as well as of different variants, the risk of redundant testing is large as well as the risk of overlooking important tests, hidden by the huge amount of possible tests.
Aims
This study assesses the amount and type of overlaid manual testing across feature, integration and system test in such context, it explores the causes of potential redundancy and elaborates on how to provide decision support in terms of visualization for the purpose of avoiding redundancy.
Method
An in-depth case study was launched including both qualitative and quantitative observations.
Results
A high degree of test overlay is identified originating from distributed test responsibilities, poor documentation and structure of test cases, parallel work and insufficient delta analysis. The amount of test overlay depends on which level of abstraction is studied.
Conclusions
Avoiding redundancy requires tool support, e.g. visualization of test design coverage, test execution progress, priorities of coverage items as well as visualized priorities of variants to support test case selection.}
}
@article{PAVLICEK20192897,
title = {Applicability and comparison of surrogate techniques for modeling of selected heating problems},
journal = {Computers & Mathematics with Applications},
volume = {78},
number = {9},
pages = {2897-2910},
year = {2019},
note = {Applications of Partial Differential Equations in Science and Engineering},
issn = {0898-1221},
doi = {https://doi.org/10.1016/j.camwa.2019.02.013},
url = {https://www.sciencedirect.com/science/article/pii/S0898122119300811},
author = {Karel Pavlíček and Václav Kotlan and Ivo Doležel},
keywords = {Induction-assisted laser welding, Surrogate modeling, Metamodeling, Computational cost reduction, Weld depth prediction, Regression analysis},
abstract = {Possibilities of using surrogate techniques for modeling selected strongly nonlinear coupled problems of heating are evaluated. The main purpose is to significantly reduce the computing time in the case of computations of many variants of a given task by the finite element method on the condition of obtaining results of a still acceptable accuracy. Frequently used surrogate techniques (based on Kriging, neural network etc.) are tested on a particular problem of induction-assisted laser welding that represents a very complicated 3D problem. Here, the most important output quantities are the internal structure of weld (decisive for its mechanical parameters) and its depth that depend on a number of input parameters (power of laser beam, velocity of shift of the welded parts, overall geometry and material properties etc.) and must be known before the process of welding itself. The paper presents both full model of this process and considered surrogate algorithms, and compares the results obtained. It is shown that a careful selection of the surrogate technique together with suitable choice of its input data is very beneficial and may result in high savings in design of the process. Implementation performance and suitability of particular techniques of this kind are also evaluated.}
}
@article{GHOUCHANI2019103360,
title = {Investigation on distal femoral strength and reconstruction failure following curettage and cementation: In-vitro tests with finite element analyses},
journal = {Computers in Biology and Medicine},
volume = {112},
pages = {103360},
year = {2019},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2019.103360},
url = {https://www.sciencedirect.com/science/article/pii/S0010482519302379},
author = {Azadeh Ghouchani and Gholamreza Rouhi and Mohammad Hosein Ebrahimzadeh},
keywords = {Distal femoral strength, In-vitro compressive tests, Quantitative computed tomography, Failure analysis, Finite element method, Non-homogeneity, Non-linear behavior},
abstract = {Cement augmentation following benign bone tumor surgery, i.e. curettage and cementation, is recommended in patients at high risk of fracture. Nonetheless, identifying appropriate cases and devices for augmentation remains debatable. Our goal was to develop a validated biomechanical tool to: predict the post-surgery strength of a femoral bone, assess the precision and accuracy of the predicted strength, and discover the mechanisms of reconstruction failure, with the aim of finding a safe biomechanical fixation. Tumor surgery was mimicked in quantitative-CT (QCT) scanned cadaveric human distal femora, and subsequently tested in compression to measure bone strength (FExp). Finite element (FE) models considering bone material non-homogeneity and non-linearity were constructed to predict bone strength (FFE). Analyses of contact, damage, and crack initiation at the bone-cement interface (BCI) were completed to investigate critical failure locations. Results of paired t-tests did not show a significant difference between FExp and FFE (P > 0.05); linear regression analysis resulted in good correlation between FExp and FFE (R2 = 0.94). Evaluation of the models precision using linear regression analysis yielded R2 = 0.89, with the slope = 1.08 and intercept = −324.16 N. FE analyses showed the initiation of damage and crack and a larger cement debonding area at the proximal end and most interior part of BCI, respectively. Therefore, we speculated that devices that reinforce critical failure locations offer the most biomechanical advantage. The QCT-based FE method proved to be a reliable tool to predict distal femoral strength, identify some causes of reconstruction failure, and assist in a safer selection of fixation devices to reduce post-operative fracture risk.}
}
@article{HSU2020,
title = {Machine Learning Model for Risk Prediction of Community-Acquired Acute Kidney Injury Hospitalization From Electronic Health Records: Development and Validation Study},
journal = {Journal of Medical Internet Research},
volume = {22},
number = {8},
year = {2020},
issn = {1438-8871},
doi = {https://doi.org/10.2196/16903},
url = {https://www.sciencedirect.com/science/article/pii/S1438887120006330},
author = {Chien-Ning Hsu and Chien-Liang Liu and You-Lin Tain and Chin-Yu Kuo and Yun-Chun Lin},
keywords = {community-acquired acute kidney injury (CA-AKI), hospitalization, treatment decision making, clinical decision support system, machine learning, feature selection with extreme gradient boost (XGBoost), least absolute shrinkage and selection operator (LASSO), risk prediction},
abstract = {Background
Community-acquired acute kidney injury (CA-AKI)-associated hospitalizations impose significant health care needs and contribute to in-hospital mortality. However, most risk prediction models developed to date have focused on AKI in a specific group of patients during hospitalization, and there is limited knowledge on the baseline risk in the general population for preventing CA-AKI-associated hospitalization.
Objective
To gain further insight into risk exploration, the aim of this study was to develop, validate, and establish a scoring system to facilitate health professionals in enabling early recognition and intervention of CA-AKI to prevent permanent kidney damage using different machine-learning techniques.
Methods
A nested case-control study design was employed using electronic health records derived from a group of Chang Gung Memorial Hospitals in Taiwan from 2010 to 2017 to identify 234,867 adults with at least two measures of serum creatinine at hospital admission. Patients were classified into a derivation cohort (2010-2016) and a temporal validation cohort (2017). Patients with the first episode of CA-AKI at hospital admission were classified into the case group and those without CA-AKI were classified in the control group. A total of 47 potential candidate variables, including age, gender, prior use of nephrotoxic medications, Charlson comorbid conditions, commonly measured laboratory results, and recent use of health services, were tested to develop a CA-AKI hospitalization risk model. Permutation-based selection with both the extreme gradient boost (XGBoost) and least absolute shrinkage and selection operator (LASSO) algorithms was performed to determine the top 10 important features for scoring function development.
Results
The discriminative ability of the risk model was assessed by the area under the receiver operating characteristic curve (AUC), and the predictive CA-AKI risk model derived by the logistic regression algorithm achieved an AUC of 0.767 (95% CI 0.764-0.770) on derivation and 0.761 on validation for any stage of AKI, with positive and negative predictive values of 19.2% and 96.1%, respectively. The risk model for prediction of CA-AKI stages 2 and 3 had an AUC value of 0.818 for the validation cohort with positive and negative predictive values of 13.3% and 98.4%, respectively. These metrics were evaluated at a cut-off value of 7.993, which was determined as the threshold to discriminate the risk of AKI.
Conclusions
A machine learning–generated risk score model can identify patients at risk of developing CA-AKI-related hospitalization through a routine care data-driven approach. The validated multivariate risk assessment tool could help clinicians to stratify patients in primary care, and to provide monitoring and early intervention for preventing AKI while improving the quality of AKI care in the general population.}
}
@article{SCHWEIER2014,
title = {A Web-Based Peer-Modeling Intervention Aimed at Lifestyle Changes in Patients With Coronary Heart Disease and Chronic Back Pain: Sequential Controlled Trial},
journal = {Journal of Medical Internet Research},
volume = {16},
number = {7},
year = {2014},
issn = {1438-8871},
doi = {https://doi.org/10.2196/jmir.3434},
url = {https://www.sciencedirect.com/science/article/pii/S1438887114000363},
author = {Rebecca Schweier and Matthias Romppel and Cynthia Richter and Eike Hoberg and Harry Hahmann and Inge Scherwinski and Gregor Kosmützky and Gesine Grande},
keywords = {coronary artery disease, lifestyle, health behavior, back pain, personal narratives as topic, Internet, diet, exercise, Web-based intervention},
abstract = {Background
Traditional secondary prevention programs often fail to produce sustainable behavioral changes in everyday life. Peer-modeling interventions and integration of peer experiences in health education are a promising way to improve long-term effects in behavior modification. However, effects of peer support modeling on behavioral change have not been evaluated yet. Therefore, we implemented and evaluated a website featuring patient narratives about successful lifestyle changes.
Objective
Our aim is to examine the effects of using Web-based patient narratives about successful lifestyle change on improvements in physical activity and eating behavior for patients with coronary heart disease and chronic back pain 3 months after participation in a rehabilitation program.
Methods
The lebensstil-aendern (“lifestyle-change”) website is a nonrestricted, no-cost, German language website that provides more than 1000 video, audio, and text clips from interviews with people with coronary heart disease and chronic back pain. To test efficacy, we conducted a sequential controlled trial and recruited patients with coronary heart disease and chronic back pain from 7 inpatient rehabilitation centers in Germany. The intervention group attended a presentation on the website; the control group did not. Physical activity and eating behavior were assessed by questionnaire during the rehabilitation program and 12 weeks later. Analyses were conducted based on an intention-to-treat and an as-treated protocol.
Results
A total of 699 patients were enrolled and 571 cases were included in the analyses (control: n=313, intervention: n=258; female: 51.1%, 292/571; age: mean 53.2, SD 8.6 years; chronic back pain: 62.5%, 357/571). Website usage in the intervention group was 46.1% (119/258). In total, 141 trial participants used the website. Independent t tests based on the intention-to-treat protocol only demonstrated nonsignificant trends in behavioral change related to physical activity and eating behavior. Multivariate regression analyses confirmed belonging to the intervention group was an independent predictor of self-reported improvements in physical activity regularity (β=.09, P=.03) and using less fat for cooking (β=.09, P=.04). In independent t tests based on the as-treated protocol, website use was associated with higher self-reported improvements in integrating physical activity into daily routine (d=0.22, P=.02), in physical activity regularity (d=0.23, P=.02), and in using less fat for cooking (d=0.21, P=.03). Multivariate regression analyses revealed that using the website at least 3 times was the only factor associated with improved lifestyle behaviors.
Conclusions
Usage of the lebensstil-aendern website corresponds to more positive lifestyle changes. However, as-treated analyses do not allow for differentiating between causal effects and selection bias. Despite these limitations, the trial indicates that more than occasional website usage is necessary to reach dose-response efficacy. Therefore, future studies should concentrate on strategies to improve adherence to Web-based interventions and to encourage more frequent usage of these programs.}
}
@article{GONZALEZPEREZ2023104359,
title = {AwarNS: A framework for developing context-aware reactive mobile applications for health and mental health},
journal = {Journal of Biomedical Informatics},
volume = {141},
pages = {104359},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104359},
url = {https://www.sciencedirect.com/science/article/pii/S1532046423000801},
author = {Alberto González-Pérez and Miguel Matey-Sanz and Carlos Granell and Laura Díaz-Sanahuja and Juana Bretón-López and Sven Casteleyn},
keywords = {mHealth, Smartphone, Data collection, Intervention, Context-awareness, Digital phenotyping},
abstract = {In recent years, interest and investment in health and mental health smartphone apps have grown significantly. However, this growth has not been followed by an increase in quality and the incorporation of more advanced features in such applications. This can be explained by an expanding fragmentation of existing mobile platforms along with more restrictive privacy and battery consumption policies, with a consequent higher complexity of developing such smartphone applications. To help overcome these barriers, there is a need for robust, well-designed software development frameworks which are designed to be reliable, power-efficient and ethical with respect to data collection practices, and which support the sense-analyse-act paradigm typically employed in reactive mHealth applications. In this article, we present the AwarNS Framework, a context-aware modular software development framework for Android smartphones, which facilitates transparent, reliable, passive and active data sampling running in the background (sense), on-device and server-side data analysis (analyse), and context-aware just-in-time offline and online intervention capabilities (act). It is based on the principles of versatility, reliability, privacy, reusability, and testability. It offers built-in modules for capturing smartphone and associated wearable sensor data (e.g. IMU sensors, geolocation, Wi-Fi and Bluetooth scans, physical activity, battery level, heart rate), analysis modules for data transformation, selection and filtering, performing geofencing analysis and machine learning regression and classification, and act modules for persistence and various notification deliveries. We describe the framework’s design principles and architecture design, explain its capabilities and implementation, and demonstrate its use at the hand of real-life case studies implementing various mobile interventions for different mental disorders used in clinical practice.}
}
@article{YUE2023120481,
title = {A comparison of six metamodeling techniques applied to multi building performance vectors prediction on gymnasiums under multiple climate conditions},
journal = {Applied Energy},
volume = {332},
pages = {120481},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.120481},
url = {https://www.sciencedirect.com/science/article/pii/S030626192201738X},
author = {Naihua Yue and Mauro Caini and Lingling Li and Yang Zhao and Yu Li},
keywords = {RDPG, A3C, LSTM, CNN, Multi vectors prediction, Gymnasiums},
abstract = {Building performance simulation (BPS) is essential for testing energy demand and indoor environment quality of different building designs. However, software for BPS is computationally intensive and impractical to run thousands even millions of simulations for performance analysis and optimization. Especially for the large space buildings, which usually have complex forms and high energy consumption. The computational problem could be overcome by the adoption of metamodels. Most correlational research focuses on single performance prediction of a particular building, which makes the model less robust when applied to multi vectors prediction for different buildings under multiple weather conditions. This paper leveraged six metamodels, which include recurrent deterministic policy gradient (RDPG), asynchronous advantage actor-critic (A3C), long short-term memory (LSTM), convolution neural network (CNN), artificial neural network (ANN) and support vector regression (SVR), to predict hourly-based multi performance vectors of gymnasiums under various design parameters and multiple weather conditions. Six metamodels are trained and tested on a large scale of datasets simulated by EnergyPlus over four gymnasium cases in different cities of China. The accuracy, efficiency, ease-of-use, robustness and interpretability of the models are investigated. To conduct a fair and detailed comparison, a methodological approach using grid searches for model settings selection assisted by sensitivity analysis is pursued. Principal component analysis (PCA) is also adopted to interpret the work process of the metamodels. The comparison showed that the RDPG model provides the most accurate prediction results with R2 converges at 0.993, 0.982 and 0.941 for energy, temperature and CO2, respectively. LSTM model is more efficient than RDPG, and suitable for users who need emphasis on both time and accuracy. ANN is suitable for users with limited time and require models of ease-of-use and robustness. SVR and ANN could be used for the automatically co-simulation with BPS software. In future research, the influence of occupant behavior also should be investigated.}
}
@article{JING2019364,
title = {A case study of TBM performance prediction using field tunnelling tests in limestone strata},
journal = {Tunnelling and Underground Space Technology},
volume = {83},
pages = {364-372},
year = {2019},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2018.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0886779817306557},
author = {Liu-jie Jing and Jian-bin Li and Chen Yang and Shuai Chen and Na Zhang and Xing-xin Peng},
keywords = {TBM, Performance prediction, Limestone, Normal force of single cutter, Penetration},
abstract = {TBM performance prediction models play important guiding roles in equipment selection, project planning, cost forecast, as well as the optimization of TBM operational parameters. In this study, field tunnelling tests were conducted in the limestone strata of Songhua River water supply project through artificially changing two operational parameters, the rotation speed (RPM) and penetration (p) of TBM cutterhead. Compared the field test and normal tunnelling data, it revealed that penetration (p) and the normal force of single cutter (Fn) had significant linear correlations with consistent rules in the two different types of penetration conditions, whereas the change of cutterhead rotation speed had little effect on the linear relationship. Based on the rules above, TBM performance prediction model for limestone strata was established after analyzing 46 sets of machine and corresponding rock mass parameters obtained during normal tunneling by using stepwise regression method. A linear relationship between the penetration and the normal force of single cutter was established initially by using the initial section data of normal tunneling cycles. Then the final model was obtained based on the analysis of the correlation between rock mass parameters and the undermined coefficients in the previous linear relationship model. The results show that the model was in good agreement with the experimental measurements. Finally, a real time rock mass state perception method based on this model was discussed. This method can be used to optimize the operational parameters and to create a foundation for intelligent control of TBMs.}
}
@article{CHUNGCHAROEN2022107019,
title = {Machine learning-based prediction of nutritional status in oil palm leaves using proximal multispectral images},
journal = {Computers and Electronics in Agriculture},
volume = {198},
pages = {107019},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.107019},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922003362},
author = {Thatchapol Chungcharoen and Irwin Donis-Gonzalez and Kittisak Phetpan and Vasu Udompetaikul and Panmanas Sirisomboon and Rattapong Suwalak},
keywords = {Machine learning, Plant nutrition, Oil palm, Multispectral Image, Image processing},
abstract = {This study evaluated the application of proximal multispectral images accompanied by 4 machine learning approaches for estimating the nutritional status of oil palm leaves. The image responded for five bands: blue, green, red, red edge, and near-infrared regions with a center wavelength of 475, 560, 668, 717, and 840 nm. Average and standard deviation (SD) values from the leaf pixels of each band were extracted, obtaining 5 average and 5 SD values from 5 bands. Thirty-four vegetation variables were generated based on those average and SD values. In total, forty-four variables consisted of 10 average-and SD-based features, and 34 vegetation variables were used as the input candidates for analyses against 10 target variables: nitrogen (N), phosphorus (P), potassium (K), calcium (Ca), magnesium (Mg), iron (Fe), manganese (Mn), zinc (Zn), boron (B), and chlorophyll (SPAD). No significant input came out for modeling with P and Zn based on the stepwise selection. Therefore, 8 nutritional models were proposed in this study. A training set with 50 samples was used to be modeled for each target, and a test set with 15 samples was employed to evaluate the models' performances. Based on random forest (RF), support vector regression (SVR), partial least square regression (PLSR), and artificial neuron network (ANN) applied to be modeled, the models for chlorophyll, N, and Ca predictions were acceptable for screening, and those for K and Mg predictions were acceptable for rough screening. The chlorophyll model developed based on the RF had the predictive statistics in terms of coefficient of determination for prediction (r2), root mean square error of prediction (RMSEP), and standard error of prediction (SEP) of 0.752, 5.46 SPAD, and 5.65 SPAD, respectively. The other 2 screening models developed based on SVR and RF for N and Ca, respectively, gave the performances with the r2, RMSEP, and SEP ranging from 0.655 to 0.718, 0.12 to 0.17%, and 0.12 to 0.18%, respectively. In the case of the 2 rough screening models established using the RF algorithm, the predictive statistics ranged from 0.496 to 0.530 for the r2 and 0.07–0.16% for both RMSEP and SEP. In this study, the Fe, Mn, and B models had poor results presenting the range of r2, RMSEP, and SEP of 0.308–0.491, 2.39–72.9 ppm, and 2.45–62.8 ppm, respectively. Based on the results, this study confirmed that the proximal multispectral information of oil palm leaves had enough significance to account for the status of chlorophyll and macro-nutrients: N, K, Ca, and Mg in the leaves.}
}
@article{MAINO2021100073,
title = {A deep neural network based model for the prediction of hybrid electric vehicles carbon dioxide emissions},
journal = {Energy and AI},
volume = {5},
pages = {100073},
year = {2021},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2021.100073},
url = {https://www.sciencedirect.com/science/article/pii/S2666546821000276},
author = {Claudio Maino and Daniela Misul and Alessandro {Di Mauro} and Ezio Spessa},
keywords = {Deep neural networks, Carbon dioxide emissions, Hybrid electric vehicles, Dynamic programming},
abstract = {Hybrid electric vehicles (HEV) are nowadays proving to be one of the most promising technologies for the improvement of the fuel economy of several transportation segments. As far as the on-road category is concerned, a wise selection of the powertrain design is needed to exploit the best energetic performance achievable by a HEV. Amongst the methodologies developed for comparing different hybrid architectures, global optimizers have demonstrated the capability of leading to optimal design solutions at the expense of a relevant computational burden. In the present paper, an innovative deep neural networks-based model for the prediction of tank-to-wheel carbon dioxide emissions as estimated by a Dynamic Programming (DP) algorithm is presented. The model consists of a pipeline of neural networks aimed at catching the correlations lying between the design parameters of a HEV architecture and the main outcomes of the DP, namely powertrain feasibility and tail pipe CO2 emissions. Moreover, an automatic search tool (AST) has been developed for tuning the main hyper-parameters of the networks. Interesting results have been registered by applying the pipeline to three databases related to three different HEV parallel architectures. The capability of the pipeline has been proved through an extensive testing campaign made up by multiple experiments. Classification performances above 91% as well as average regression errors below 1% have been achieved during an extensive set of simulations. The presented model could hence be considered as an effective tool for supporting HEV design optimization phases.}
}
@article{YI201589,
title = {An integrated energy–emergy approach to building form optimization: Use of EnergyPlus, emergy analysis and Taguchi-regression method},
journal = {Building and Environment},
volume = {84},
pages = {89-104},
year = {2015},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2014.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S0360132314003345},
author = {Hwang Yi and Ravi S. Srinivasan and William W. Braham},
keywords = {Energy simulation, Emergy evaluation, Building form optimization, Taguchi-ANOVA method},
abstract = {This research presents a new methodology of environmental building design with an integrated energy–emergy (spelled with an “m”) approach to study building form optimization in the schematic phases. In architecture, selection of sustainability assessment methods critically affects design goals, favoring or restricting choices designers can make. A building subsumes matter and energy to support human lives, but current building performance indicators are still hard to equate technical sides and human dominant sides of various scales in a synthetic metric. Moreover, in order to achieve global sustainability in a building, as a part of the whole built environment, it is necessary to integrate energy and environmental impacts at the highest scope of analysis. Emergy analysis coupled with building energy simulation can be suggested as a holistic indicator for architectural design process. To test the proposed method, a pilot study with a mid-size office building evaluates the consequences of early design decisions such as basic geometry, aspect ratio, window-wall ratio, construction types, etc. The integrated energy–emergy approach to building form optimization consists of three modules namely, Building Energy Simulation (BES) module, Building EMergy Analysis (BEMA) module, and (iii) MetaModel Development (MMD) module. The BES module uses the EnergyPlus tool for whole building energy analysis, while the BEMA module employs analytical methods to estimate emergy quantities, and the MMD module employs the Taguchi method to develop a metamodel for faster and easier whole building emergy simulation. The metamodel developed using Taguchi-ANOVA method for building form optimization was validated with analytical test results to accelerate environmental design decision-making. This study demonstrates possibility of wider applications of emergy synthesis to building energy research and facilitates practical use of emergy simulation in the environmental design process.}
}
@article{MOON2018129,
title = {Computer-aided prediction model for axillary lymph node metastasis in breast cancer using tumor morphological and textural features on ultrasound},
journal = {Computer Methods and Programs in Biomedicine},
volume = {162},
pages = {129-137},
year = {2018},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2018.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S0169260717312919},
author = {Woo Kyung Moon and I-Ling Chen and Ann Yi and Min Sun Bae and Sung Ui Shin and Ruey-Feng Chang},
keywords = {Breast cancer, Lymph node metastasis, Ultrasound, Computer-aided prediction, Axillary lymph node},
abstract = {Background and objectives
Axillary lymph node (ALN) status is a key indicator in assessing and determining the treatment strategy for patients with newly diagnosed breast cancer. Previous studies suggest that sonographic features of a primary tumor have the potential to predict ALN status in the preoperative staging of breast cancer. In this study, a computer-aided prediction (CAP) model as well as the tumor features for ALN metastasis in breast cancers were developed using breast ultrasound (US) images.
Methods
A total of 249 malignant tumors were acquired from 247 female patients (ages 20–84 years; mean 55 ± 11 years) to test the differences between the non-metastatic (130) and metastatic (119) groups based on various features. After applying semi-automatic tumor segmentation, 69 quantitative features were extracted. The features included morphology and texture of tumors inside a ROI of breast US image. By the backward feature selection and linear logistic regression, the prediction model was constructed and established to estimate the likelihood of ALN metastasis for each sample collected.
Results
In the experiments, the texture features showed higher performance for predicting ALN metastasis compared to morphology (Az, 0.730 vs 0.667). The difference, however, was not statistically significant (p-values > 0.05). Combining the textural and morphological features, the accuracy, sensitivity, specificity, and Az value achieved 75.1% (187/249), 79.0% (94/119), 71.5% (93/130), and 0.757, respectively.
Conclusions
The proposed CAP model, which combines textural and morphological features of primary tumor, may be a useful method to determine the ALN status in patients with breast cancer.}
}
@article{WESTCOTT2022,
title = {Prediction of Maternal Hemorrhage Using Machine Learning: Retrospective Cohort Study},
journal = {Journal of Medical Internet Research},
volume = {24},
number = {7},
year = {2022},
issn = {1438-8871},
doi = {https://doi.org/10.2196/34108},
url = {https://www.sciencedirect.com/science/article/pii/S1438887122004769},
author = {Jill M Westcott and Francine Hughes and Wenke Liu and Mark Grivainis and Iffath Hoskins and David Fenyo},
keywords = {predictive modeling, maternal morbidity, postpartum hemorrhage, machine learning, obstetrics, pregnancy, post partum, maternal},
abstract = {Background
Postpartum hemorrhage remains one of the largest causes of maternal morbidity and mortality in the United States.
Objective
The aim of this paper is to use machine learning techniques to identify patients at risk for postpartum hemorrhage at obstetric delivery.
Methods
Women aged 18 to 55 years delivering at a major academic center from July 2013 to October 2018 were included for analysis (N=30,867). A total of 497 variables were collected from the electronic medical record including the following: demographic information; obstetric, medical, surgical, and family history; vital signs; laboratory results; labor medication exposures; and delivery outcomes. Postpartum hemorrhage was defined as a blood loss of ≥1000 mL at the time of delivery, regardless of delivery method, with 2179 (7.1%) positive cases observed. Supervised learning with regression-, tree-, and kernel-based machine learning methods was used to create classification models based upon training (21,606/30,867, 70%) and validation (4630/30,867, 15%) cohorts. Models were tuned using feature selection algorithms and domain knowledge. An independent test cohort (4631/30,867, 15%) determined final performance by assessing for accuracy, area under the receiver operating curve (AUROC), and sensitivity for proper classification of postpartum hemorrhage. Separate models were created using all collected data versus models limited to data available prior to the second stage of labor or at the time of decision to proceed with cesarean delivery. Additional models examined patients by mode of delivery.
Results
Gradient boosted decision trees achieved the best discrimination in the overall model. The model including all data mildly outperformed the second stage model (AUROC 0.979, 95% CI 0.971-0.986 vs AUROC 0.955, 95% CI 0.939-0.970). Optimal model accuracy was 98.1% with a sensitivity of 0.763 for positive prediction of postpartum hemorrhage. The second stage model achieved an accuracy of 98.0% with a sensitivity of 0.737. Other selected algorithms returned models that performed with decreased discrimination. Models stratified by mode of delivery achieved good to excellent discrimination but lacked the sensitivity necessary for clinical applicability.
Conclusions
Machine learning methods can be used to identify women at risk for postpartum hemorrhage who may benefit from individualized preventative measures. Models limited to data available prior to delivery perform nearly as well as those with more complete data sets, supporting their potential utility in the clinical setting. Further work is necessary to create successful models based upon mode of delivery and to validate the findings of this study. An unbiased approach to hemorrhage risk prediction may be superior to human risk assessment and represents an area for future research.}
}